{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "개요\n",
    "- **Support Vector Machine (SVM)**은 classification의 한 방법론으로 1990대 computer science community가 발전시켰다. \n",
    "- SVM은 **maximal margin classifier**(9.1)의 개념을 확장 및 일반화한 support vector classifier이다. \n",
    "- Maximal margin classifier는 class가 linear boundary로 구분 가능해야 하기 때문에 데이터에 적용하기 어렵다.\n",
    "- 이러한 단점을 보완하고 조금 더 다양한 데이터에 적용할 수 있도록 **support vector classifier**(9.2)가 발전했다. \n",
    "- 여기서 비선형(non-linear) class boundary까지 고려한 모형이 바로 **SVM**이다 (9.3). \n",
    "- SVM은 binary classifier이기 때문에 이진변수뿐만 아니라 categorical variable (multiple classes)까지 확장하여 고려할 수 있다. (9.4) \n",
    "- Logistic regression등 statistics community에서 도출한 모형과 SVM은 밀접한 관계가 있다. (9.5)\n",
    "\n",
    "*상기 세 가지 모형을 모두 support vector machines라고 부르지만 세 가지 모형의 차이점을 알아둘 필요가 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1 Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.1 What is a hyperplane?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*hyperplane*: Flat affine subspace of dimension p-1. 여기서 affine은 원점을 통과하지 않아도 됨을 의미한다. 3차원에서 hyperplane은 면이고, 2차원에서는 선이다. 이를 수학적으로 정의하면 다음과 같다. \n",
    "<p> $\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2+...+\\beta_{p}X_p=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.2 Classification using a Separating Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification을 위해 linear discriminant analysis, logistic regression, classification trees, bagging, boosting 등을 배웠다. 이번 장은 기본적으로 hyperplane을 쪼개서 classify하는 방법을 배울 것이다. Hyperplane을 separate하여 classification을 하는 핵심 idea는 다음과 같다. \n",
    "<br> \n",
    "**Setting:**\n",
    "- nxp행렬 **X** (p-차원 변수공간)\n",
    "<br> <p> $x_1=\\begin{pmatrix}x_{11}\\\\...\\\\x_{1p}\\end{pmatrix},...,x_n=\\begin{pmatrix}x_{n1}\\\\...\\\\x_{np}\\end{pmatrix}$ $\\in$ **$R$**$^p$ <br><br>\n",
    "-  각각 observation은 두 개의 class 중 하나에 속한다. \n",
    "<br> <p> $y_1,...,y_n \\in \\{-1, 1\\}$ \n",
    "<br><br>\n",
    "- test observation $x^* = (x_1^* ... x_p^*)^T$\n",
    "<br><br>\n",
    "**Assumption:**\n",
    "- Hyperplane을 가지고 모든 training observation을 class label별로 분리 가능해야 한다는 가정이 필요하다. (9.2 좌측 그래프)\n",
    "<br><br>\n",
    "**Example:**\n",
    "- 아래 그림 9.2에서 파란색 class에 속한 observation $y_i=1$, 보라색 class에 속한 observation $y_i=-1$로 구분 (9.2)\n",
    "- 여기서 separating hyperplane은 다음과 같은 성질을 가지고 있다. \n",
    "<br><p> $\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2+...+\\beta_{p}X_p>0$ if $y_i=1$\n",
    "<br><p> $\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2+...+\\beta_{p}X_p<0$ if $y_i=-1$<br><br>\n",
    "또는\n",
    "<br><p> $y_i(\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2+...+\\beta_{p}X_p)>0$<br><br>\n",
    "- 만약 separating hyperplane이 존재하면 classifer를 만들 수 있다. test observation $x^*$를 함수<br>\n",
    "<br> $f(x)=\\beta_{0}+\\beta_{1}x_1+\\beta_{2}x_2+...+\\beta_{p}x_p>0$에 대입하여, $f(x^*)>0$이면 $x^*$을 1로 classify,$f(x^*)<0$이면 $x^*$를 -1로 classify한다. <br><br>\n",
    "- 여기서 $f(x^*)$의 magnitude(크기)를 사용할 수 있는데, 절대값의 크기가 0보다 많이 큰 경우 $x^*$은 hyperplane과 멀다는 얘기고, $x^*$의 classification에 대해 더욱 더 신뢰할 수 있을 것이다. 반대로 절대값 크기가 0이랑 가까운 경우 $x^*$은 hyperplane과 가깝다는 얘기이며 $x^*$의 classification에 대해 더욱 더 신뢰하기 어려울 것이다. \n",
    "- decision boundary가 linear한 것을 알 수 있다. (9.2 우측 그래프)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 9.2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.3 The Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense한 Real space에서 만약 data를 완벽하게 separate 할 수 있는 hyperplane이 하나라도 존재한다면, 이 hyperplane의 각도를 $\\epsilon$ 만큼 이동하거나 $\\epsilon$만큼 평행이동하게 되면 무수히 많은 hyperplane을 찾을 수 있을 것이다. 그렇기 때문에 일단 separating hyperplane이 존재한다고 가정하고 무한히 많은 hyperplane 중 가장 좋은 hyperplane을 고르는 것이 이번 장의 목적이며, 여기서 뽑아낸 hyperplane으로 test observation을 classify하는 범위를 정해주면 maximal margin classifier (optmal separating hyperplane)가 된다. \n",
    "<br><br>\n",
    "Optimal separable hyperplane: training observation과 가장 먼 hyperplane\n",
    "1. minimize the perpendicular distance: 각 separating hyperplane에 대해 hyperplane으로부터의 수직거리가 가장 짧은 (수직거리를 가장 최소화하는) observation을 찾는다. 해당 거리를 margin이라 칭한다. \n",
    "2. maximize the margin: 각 separating hyperplane마다 margin이 있을 것인데, 이 margin을 최대화해주는 hyperplane이 바로 **maximal margin hyperplane**이다. \n",
    "<br><br>\n",
    "두 class 사이를 가르는 나무판의 너비를 최대화 했을때, 나무판의 중앙선이 바로 maximal margin hyperplane이다. (9.3)\n",
    "<br><br>\n",
    "위 9.1.2의 Example에 나와있는 함수와 같이 각 test observation $x^*$는 $f(x^*)$의 부호에 따라 classify가 될 것이다. \n",
    "\n",
    "**support vectors**: margin이 maximal margin hyperplane의 margin인 모든 observation벡터. 즉, 나무판 옆라인에 위치한 모든 점(더 엄밀하게는 vector). 이 점들이 이동하면 maximal margin hyperplane도 이동하기 때문에 maximal margin hyperplane은 support vector에 의존을 하지만, margin을 기준으로 한 경계선을 넘지 않는한 다른 점들에 의존하지 않는다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 9.3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.4 Construction of the Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 Maximal Margin Hyperplane을 도출하는 방법을 optimization problem으로 표현할 수 있다. \n",
    "<br>\n",
    "<br>\n",
    "Let $x_1,...,x_n \\in$ **$R$**$^p$ and $y_1,...,y_n \\in \\{-1,1\\}$. <br>\n",
    "<br>\n",
    "$\\max\\limits_{\\beta_0,...\\beta_p}M$ (9.9)<br> subject to $\\sum_{j=1}^{p}\\beta_j^2=1$, (9.10) &nbsp;&nbsp;&nbsp;$y_i(\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2+...+\\beta_{p}X_p)\\geq M$ (9.11) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\forall$ $i = 1,...n$, &nbsp; $M>0$\n",
    "<br><br>\n",
    "9.11은 사이 거리가 margin M이 되도록하는 hyperplane 두 개를 형성한다. 9.10의 조건으로 인해 9.11의 LHS에 있는 부분이 i번째 observation과 hyperplane 사이의 수직 거리가 된다고 책에서 말하지만 이 부분에 대해서 이해가 조금 부족하다. 9.10은 beta를 p-차원 vector로 보면 ||**$\\beta$**|| norm을 표현한 것 같고, 9.11의 LHS은 $y_i$와$\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2+...+\\beta_{p}X_p$의 내적인 것 같다. 9.10과 9.11의 조건은 각 observation에 대해 hyperplane을 사용하여 옳은 위치로 구분하고 최소 M만큼의 거리를 유지하도록 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.5 Non-separable Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 아래 Figure 9.4처럼 separating hyperplane이 존재하지 않을 수 있으며, 상기 최적화 문제에 대한 해를 구할 수 없을 것이다. 따라서 non-separable case인 경우까지 확장하여 best hyperplane을 선택해야 하고 이에 대한 기준도 필요하다. 이때 **soft margin**을 이용하여 non-separable case까지 고려한 방법이 바로 **support vector classifier**이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 9.4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2 Support Vector Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.1 Overview of the Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-separable한 경우 이외에도 support vector classifier가 maximal margin classifier보다 성능이 좋을때가 있다. 후자를 이용해 모든 observation을 완벽하게 classify하면 classifier는 개별 observation에 대해 민감해질 수 있고 (sensitivity) 이 때문에 train set에 대해 overfitting이 발생할 수 있다 (9.5).  Support vector classifier (soft margin classifier)은 다음과 같은 측면을 고려하여 data를 완벽하게 구분짓지 않는 hyperplane을 사용한다. \n",
    "- 개별 observation에 대해 더 강한 robustness\n",
    "- 모든 training observation이 아닌, **거의** 모든 training observation에 대한 classification\n",
    "<br>\n",
    "<br>\n",
    "따라서 support vector classifier을 사용하는 경우 observation이 반대쪽 margin에 있거나 반대쪽 hyperplane에 위치할 수도 있다. (9.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 9.5.png\">\n",
    "<img src=\"pic/Figure 9.6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.2 Details of the Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "$\\max\\limits_{\\beta_0,...\\beta_p,\\epsilon_1,...,\\epsilon_n}M$ (9.12)<br><br> subject to $\\sum_{j=1}^{p}\\beta_j^2=1$, (9.13) <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$y_i(\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2+...+\\beta_{p}X_p)\\geq M(1-\\epsilon_i)$, (9.14) \n",
    "<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\epsilon_i \\geq 0$, &nbsp; $\\sum_{i=1}^{n}\\epsilon_i \\leq C$, (9.15)\n",
    "<br><br>$\\forall$ $i = 1,...n$, &nbsp; $M>0$\n",
    "<br><br>\n",
    "위 최적화 문제를 풀고 각 test observation $x^*$는 $f(x^*)$의 부호에 따라 classify가 될 것이다. \n",
    "<br><br>\n",
    "slack variables:$\\epsilon_1,...,\\epsilon_n$<br>\n",
    "- $\\epsilon_i=0$인 경우 i번째 observation은 올바른 margin에 위치해 있다.<br>\n",
    "- $\\epsilon_i>0$인 경우 i번째 observation은 다른 margin에 위치해 있다.<br>\n",
    "- $\\epsilon_i>1$인 경우 i번째 observation은 반대쪽 hyperplane에 위치해 있다.<br>\n",
    "<br>\n",
    "C는 tuning parameter이며 C=0이 benchmark case이다. C개의 observation이 반대쪽 hyperplane에 위치해 있으면 안 된다. 반대쪽 hyperplane에 있을 경우 $\\epsilon_i>1$이기 때문에 9.15식에서 오른쪽 부등식의 LHS는 C보다 커져서 모순이 생기기 때문이다. C가 클수록 margin을 더 크게 잡아 다른 margin에 위치한 observation이 더 많아진다. C의 조정은 bias-variance trade-off과 같은 문제이기 때문에 cross-validation을 통해 최적의 C를 도출할 수 있다. C가 작은 경우 low bias, high variance 상황이며 C가 큰 경우 high bias, low variance 상황이 된다. \n",
    "<br><br>\n",
    "Support Vector Classifier도 마찬가지로 margin 상에 또는 다른 margin에 위치한 observation은 hyperplane에 영향을 미치지만 올바른 margin에 위치한 observation이 올바른 margin에 있기만 하면 이동해도 hyperplane에 영향을 미치지 않는다. Support vector는 margin 상에 위치하기 때문에 hyperplane에 영향을 준다. C가 큰 경우 observation 중 support vector가 많아서 low variance를 갖지만 high bias가 있다 (9.7 top-left). C가 큰 경우 support vector가 적어 high variance를 갖지만 low bias가 있다 (9.7 bottom-right). Support Vector Classifier가 작은 벡터 집합인 support vector에 의해 결정된다는 것은 hyperplane과 멀어진 observation에 대해서는 robust함을 의미한다 (cf. LDA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 9.7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3 Support Vector Machine\n",
    "\n",
    "먼저 선형 분류기를 비선형 decision boundary를 생성하는 분류기로 변환하는 일반적인 메커니즘에 대해 논의한 후, 이를 자동으로 수행하는 서포트 벡터 머신을 소개하겠다.\n",
    "\n",
    "## 9.3.1 Classiﬁcation with Non-linear Decision Boundaries\n",
    "\n",
    "9.2의 svc은 두 클래스 간의 경계가 선인 경우의 분류를 위한 자연스러운 접근 방식이지만, 우리는 살면서 비선형적 경계의 클래스들과 마주한다.\n",
    "<br>\n",
    "- 이런 경우 어떻게 나눌 수 있을 것인가? 우리는 7장에서 비슷한 상황에 직면했다. 예측변수와 결과 사이의 관계가 비선형일때, 선형 회귀의 성능이 저하될 수 있기 때문에 비선형성을 해결하기 위하여 2차, 3차와 같은 예측변수를 사용하였다. \n",
    "<br>\n",
    "- suppor vector의 경우 역시 2차, 3차 같은 예측변수를 사용하여 feature space 확대를 고려할 수 있다. 한마디로 다항식 함수를 사용하여 해결할 수 있다.\n",
    "\n",
    "<img src='https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/9.3.1.JPG?raw=true' style=\"zoom:67%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2684/1*3t_Gn5yuirT6fSC-sbxKAA.png\" style=\"zoom:50%;\" />\n",
    "\n",
    "이가 비선형 decision boudary로 이어지는 이유는?\n",
    "<br>\n",
    "- 확대된 feature space에서 (9.16)에서 발생하는 결정 경계는 실제로 선형이지만, 원래 특징 공간에서 결정 경계는 $q(x) = 0$형식이며, 여기서 q는 2차 다항식이고 그 해는 일반적으로 비선형이다. 추가로 고차 다항식 항 또는 $XjXj'$ ($j$ is not $j'$) 형식의 상호작용항을 사용하여 특성 공간을 확대할 수 있다. \n",
    "- 또는 다항식이 아닌 예측 변수의 다른 함수를 고려할 수 있다. 다만 계산을 관리하기 쉬운 방법이어야 하는데, 다음에 소개할 svm은 svc에서 사용하는 feature space를 효율적으로 계산하는 방법으로 확장이 가능하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.2 Support Vector Machine\n",
    "SVM(Support Vector Machine)은 kernel을 사용하여 특정 방식으로 feature space를 확대하여 생성되는 svc의 확장이다. \n",
    "<br>커널이 다소 복잡하고 이 책의 범위를 벗어나지만, 주요 아이디어는 클래스간 비선형 경계의 수용을 위한 feature space의 확장이다.\n",
    "<br>Input space에서 차원을 확장한 Feature space로 초평면을 그리면, 선형으로 클래스를 분류할 수 있는 것이다. (단, 보고는 저차원으로 환원)\n",
    "\n",
    "**kernel ftn: 1) 고차원으로 만들고 2) 점간의 유사도를 계산한다. 즉 확장된 feature의 내적이다.** 이때 내적이 유사도인 이유는 코사인 유사도 때문\n",
    "<img src=\"https://goofcode.github.io/assets/img/%E1%84%8B%E1%85%A7%E1%84%85%E1%85%A5%20%E1%84%80%E1%85%A1%E1%84%8C%E1%85%B5%20%E1%84%8B%E1%85%B2%E1%84%89%E1%85%A1%E1%84%83%E1%85%A9%20%E1%84%8E%E1%85%B3%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8%20(Similarity%20Measure)/download.png\"  style=\"zoom:67%;\" />\n",
    "<img src=\"https://goofcode.github.io/assets/img/%E1%84%8B%E1%85%A7%E1%84%85%E1%85%A5%20%E1%84%80%E1%85%A1%E1%84%8C%E1%85%B5%20%E1%84%8B%E1%85%B2%E1%84%89%E1%85%A1%E1%84%83%E1%85%A9%20%E1%84%8E%E1%85%B3%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8%20(Similarity%20Measure)/cosinesimilarityfq1.png\"  style=\"zoom:67%;\" />\n",
    "\n",
    "<br>kernel approach는 이 아이디어를 실행하기 위한 효율적 계산 방식이다. 책의 범위를 벗어나는 부분은 설명하지 않겠지만, 고차원에서의 내적이라고 이해하면 편하다. \n",
    "- 두 개의 r-vectors a와 b의 내적은 <a, b> = $\\sum_{i=1}^{r} a_i b_i $로 정의된다. \n",
    "<br>\n",
    "- 따라서 두 관측치 $xi, xi'$의 내적은 다음과 같이 <$x_i, x_i'$> = $\\sum_{i=1}^{r} x_ij x_ij'$로 정의된다. \n",
    "- 선형 svc는 다음과 같이 표현할 수 있게 된다. n은 파라미터의 개수다.\n",
    "<br>\n",
    "$$f(x)=B_0+\\sum_{i=1}^{n}a_i<x,x_i>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> 여기서 범위를 S 즉 support point 애들로 바꾸면 $f(x)=B_0+\\sum_{S}a_i<x,x_i>$\n",
    "\n",
    "커널 함수를 일반화하면 $K<x_i, x_i'>$ = $\\sum_{j=1}^{r} x_ij x_i'j$처럼 된다.\n",
    "\n",
    "\n",
    "- Polynomial Kernel(다항식 커널)의 경우 다음과 같다. $$K<x_i, x_i'>=(c+\\sum_{j=1}^{r} x_ij x_i'j)^d$$ \n",
    "<br>이때 d는 확장 차원이고, 앞에 붙은 c는 축을 늘리려는 목적이다. 즉 c가 크면 축이 늘어난다. 그럼 f(x)는 이렇게 될 것이다. \n",
    "<br>\n",
    "$$f(x)=B_0+\\sum_{i=1}^{n}a_i K(x,x_i)$$\n",
    "\n",
    "- Radial Kernel도 있는데, 얘는 $$K(x_i, x_i') = exp(−γ\\sum_{j=1}^{p}(x_ij-xi'j)^2$$\n",
    "와 같이 생긴 커널을 사용한다. 잘 보면 노말분포 pdf랑 닮은 것을 알 수 있다. \n",
    "γ이 크면 Big Variance, 작으면 Big Bias.\n",
    "- 중요한 점은 이 커널의 경우 무한차원 확장이 가능하다는 점이다. 이는 talor expansion을 생각했을때, exponential이 아주 많은 계산을 표현할 수 있기 때문이다. \n",
    "- 무한차원 확장의 장점은 아무리 굴곡진 Discriminant Function이 필요한 데이터더라도, 무한차원에서는 곧은 hyper plane을 그을 것이라 상상이 가능하기 때문이다.\n",
    "<img src='https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/9.3.2.JPG?raw=true'  style=\"zoom:67%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.3 An Application to the Heart Disease Data\n",
    "8 장에서는 decision tree 및 관련 방법을 심장병 데이터에 적용한다. 개인의 심장 질환 여부를 예측하기 위해 연령, 성별과 같은 13 개의 예측 변수를 사용하는 것이. 이제 이 데이터에서 SVM이 LDA와 어떻게 비교되는지 조사해보자.\n",
    "<br><br>\n",
    "<img src=\"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/9.3.2.2.JPG?raw=true\" style=\"zoom:67%;\" />\n",
    "데이터는 297 명의 피험자로 구성되어 있으며 무작위로 207 개의 훈련과 90 개의 테스트 관찰로 나눈다. \n",
    "먼저 LDA와 SVC를 훈련 데이터에 맞춘다. SVC는 차수가 d = 1인 다항식 커널을 사용하는 SVM과 동일하다.\n",
    "<br><br> 그림 9.10의 왼쪽 패널에는 LDA 및 SVC 모두의 train set 예측에 대한 ROC 곡선이 표시된다. 두 분류기 모두 점수를 계산한다. 각 관측 값에 대해 $f(X) = B_0 +B_1 X_1 + β_2 X_2 + ... + B_pX_p$ 형식이다. 주어진 컷오프 t에 대해 $f(X) <t$ 또는 $f(X) ≥ t$ 인지에 따라 관찰을 심장질환 있음 또는 심장질환 없음 범주로 분류한다. \n",
    "<br><br>ROC 곡선은 이러한 예측을 형성하고 t 값 범위에 대해 false positive 및 true positive 비율을 계산하여 얻는다. 최적의 분류기는 ROC 플롯의 왼쪽 상단 모서리를 포옹한다. 이 경우 LDA와 지원 벡터 분류기는 모두 잘 수행되지만 SVC가 약간 더 우수할 수 있다.\n",
    "<br><br>\n",
    "그림 9.10의 오른쪽 패널에는 다양한 γ 값과 함께 radial kernel을 사용하는 SVM에 대한 ROC 곡선이 표시된다. γ가 증가하고 적합도가 더 비선형이되면 ROC 곡선이 향상된다. γ = 0.1을 사용하면 거의 완벽한 ROC 곡선을 제공하는 듯하다. 그러나 이러한 곡선은 새로운 test 데이터의 성능 측면에서 잘못 이해할 수 있는 train error을 나타낸다. \n",
    "<img src=\"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/9.3.3.JPG?raw=true\"  style=\"zoom:67%;\" />\n",
    "그림 9.11은 90개의 테스트 관찰에서 계산된 ROC 곡선을 보여주는데, 훈련 ROC 곡선과 약간의 차이가 있다. 그림 9.11의 왼쪽 패널에서 SVC는 LDA에 비해 좀 나아 보이지만, 통계적으로 유의한 정도는 아니다. 오른쪽 패널에서 훈련 데이터에서 최상의 결과를 보여준 0.1을 사용하는 SVM은 테스트 데이터에서 최악의 추정치를 생성한다. 이것은 더 유연한 방법이 종종 더 낮은 train error을 생성하지만 이것이 반드시 테스트 데이터의 성능을 향상시키는 것은 아니라는 증거다. γ = 0.01 인 SVM및 γ = 0.001은 SVC와 비교하여 성능이 비슷하며, 세 가지 모두 γ = 0.1.인 SVM을 능가한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.4 SVMs with More than Two Classes\n",
    "지금까지 우리의 논의는 이진 분류, 즉 2 클래스 설정의 분류로 제한되었다. 임의의 수의 클래스가 있는 보다 일반적인 경우로 SVM을 어떻게 확장 할 수 있을까?<br>SVM이 기반으로하는 초평면 분리 개념은 자연스럽게 두 개 이상의 클래스에 적합하지 않은 것으로 나타났다. SVM을 K 클래스 케이스로 확장하기 위한 여러 제안이 있었지만 가장 인기 있는 두 가지는 일대일 접근 방식과 일대다 접근 방식이다.\n",
    "\n",
    "## 9.4.1 One-Versus-One Classification\n",
    "SVM을 사용하여 분류를 수행하는데  $K> 2$ 개의 클래스가 있다고 가정하자. one versus one 또는 all-pairs approach는 각각 한 쌍의 클래스를 비교하는 $\\binom{K}{2}$ SVM을 구성한다. \n",
    "<br>예를 들어, 그러한 SVM 중 하나는 +1로 코딩된 k번째 클래스를 −1로 코딩된 k'번째 클래스와 비교할 수 있다. 각  $\\binom{K}{2}$ classifier를 사용하여 test observation을 분류하고, test observation이 K개 클래스 각각에 할당된 횟수를 집계한다. 최종 분류는 이러한  $\\binom{K}{2}$ 분류에서 가장 빈번하게 할당된 클래스에 test observation을 할당하여 수행된다.\n",
    "\n",
    "## 9.4.2  One-Versus-All Classification\n",
    "일대다 접근 방식은 K> 2 클래스의 경우 SVM을 적용하는 또 다른 방법이다. 모든 K 클래스 중 하나를 나머지 K-1개의 클래스와 비교할 때마다 K SVM을 적합한다. $β_0k, β_1k, ..., β_pk$는 k번째 클래스(+1로 코딩 됨)를 다른 클래스(-1로 코딩 됨)와 비교하여 SVM을 피팅하여 얻은 매개 변수를 나타낸다. <br> 우리는 observation을 $β_0k + β_1k x^*+....+β_pk x_p^*$ 의 식을 가장 크게 만드는 클래스에 할당한다. 이것은 해당 관찰이 다른 클래스가 아닌 k번째 클래스에 속한다는 신뢰감을 준다. ( $x^*$는 test observation을 나타낸다.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.5 Relationship to Logistic Regression\n",
    "SVM이 1990 년대 중반에 처음 도입되었을 때 통계 및 머신러닝 커뮤니티에서 큰 인기를 얻었다. 이는 부분적으로 그들의 좋은 성과, 좋은 마케팅, 그리고 근본적인 접근 방식이 참신하고 신비스러워 보이기 때문이다. 가능한 한 데이터를 분리하는 초평면을 찾는 동시에 이러한 분리에 대한 일부 위반을 허용한다는 아이디어는 로지스틱 회귀 및 선형 판별 분석과 같은 분류에 대한 고전적인 접근 방식과는 완전히 다른 것처럼 보였다. 또한 커널을 사용하여 확장한다는 아이디어는, 비선형 클래스 경계를 수용하기위한 feature space는 독특한 특성으로 나타났다.\n",
    "<br><br>그러나 그 이후로 SVM과 다른 보다 고전적인 통계 방법이 등장했습니다. \n",
    "<br>하나는 SVC를 맞추기 위한 기준 (9.12) – (9.15)를 다시 썼을 때 밑에와 같이 나타나는 방법이다. \n",
    "$f(X) = β_0 + β_1X_1 + ... + β_pX_p$ as\n",
    "<img src=\"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/9.5.1.JPG?raw=true\" style=\"zoom:67%;\" />\n",
    "여기서 λ는 nonnegative tunning parameter다. λ가 클 때 β1, ..., βp는 작으며 margin에 대한 더 많은 위반이 허용되며, 낮은 var이지만 높은 bias인 classifier가 된다. λ가 작으면 margin에 대한 위반이 거의 발생하지 않는다. 이것은 var가 높지만 bias가 낮은 classifier다. \n",
    "<br>따라서 (9.25)에서 λ의 작은 값은 (9.15)에서 C의 작은 값에 해당합니다. (9.25)의 λ 항은 6.2.1에서의 ridge 패널티 항과 비슷한 것으로, SVC의 bias-variance trade-off를 제어하는 데 비슷한 역할을 하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 (9.25)는 우리가 반복적으로 본 \"Loss + Penalty\"형식을 취한다. 이 책 전체에서 : $minimize$ ${L(X, y, β) + λP(β)}$ \n",
    "<br>\n",
    "위 식에서 L(X,y,β)는 β로 매개변수화 된 모델이 데이터 (X, y)에 적합한 정도를 정량화하는 손실 함수이고, 베타가 nonnegative tunning parameter 람다에 의해 제어되는 벡터일때, P(β)는 파라미터에 대한 패널티 함수다. \n",
    "<br><br>예를 들어 ridge와 lasso 회귀는 모두 다음과 같은 형태다. \n",
    "<img src=\"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/9.5.1.2.JPG?raw=true\" style=\"zoom:67%;\" />\n",
    "ridge regresion에서는 P(β)가 $β_j$를 제곱해서 모두 더한 꼴이고, lasso에서는 절댓값을 모두 더한 꼴이라는 차이점은 있다. \n",
    "9.25의 케이스는 릿지와 라소 모두 다음과 같은 꼴을 취하게 된다. 이를 Hinge Loss라고하며 그림 9.12에 나와 있다. \n",
    "<img src=\"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/9.5.1.3.JPG?raw=true\" style=\"zoom:67%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 Hinge Loss는 9.12에서도 볼 수 있듯, Hinge Loss 함수가 로지스틱 회귀에 사용되는 Loss 함수와 밀접한 관련이 있다.\n",
    "<img src=\"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/9.5.1.4.JPG?raw=true\" style=\"zoom:67%;\" />\n",
    "- SVC의 흥미로운 점은 support vector가 obtained된 classifier에서만 play된다는 점이다.  이는 그림 9.12에 표시된 손실 함수가 yi(β0 + β1xi1 + ... + βpxip) ≥ 1인 관측치에 대해 0이라는 점 때문이다. 이것이 바로 margin의 correct side에 있는 observation에 해당하므로, margin의 correct side에 대한 observation은 영향을 주지 않는다.\n",
    "<br><br>\n",
    "- 대조적으로 그림 9.12에 표시된 로지스틱 회귀의 손실 함수는 정확히 0이 아니다. 그러나 그것은 decision boundary에서 멀리 떨어진 관측치의 경우 작기는 하다. 손실 함수 간의 유사성으로 인해 로지스틱 회귀와 SVM은 종종 매우 유사한 결과를 제공한다. \n",
    "<br><br>\n",
    "- *결론: 클래스가 잘 분리되면 SVM이 로지스틱 회귀보다 더 잘 작동하는 경향이 있고, 클래스가 더 겹치는 경우는 로지스틱 회귀가 종종 선호된다.*\n",
    "\n",
    "\n",
    "- SVM이 처음 도입되었을 때 (9.15)의 튜닝 파라미터 C는 1 같은 일부 기본값으로 설정할수 있는 중요하지 않은,“nuisance”parameter라고 생각했다. <br>그러나 \"Loss + Penalty\" SVM에 대한 공식(9.25)은 이것이 사실이 아님을 보여준다. 튜닝 파라미터의 선택은 중요하고, 그림 9.7에서 설명한것처럼 모델이 데이터를 과적합하는 정도를 결정한다. 우리는 이를 통해 SVM이 Logistic Regression 및 기타 기존의 통계방법들과 연관이 있음을 확인했다. \n",
    "<br><br>\n",
    "- 그렇다면 SVM은 비선형 클래스 경계를 수용하기 위해 feature space를 확장하고 kernel을 사용하는 고유한 방법일까? 답은 아니다. 우리는 비선형 커널을 사용하여 로지스틱 회귀는 물론 이 책의 다른 분류방법들을 수행할 수 있다. 이는 7장의 non linear 접근법과도 밀접한 관련이 있다. 다만 역사적인 이유로, 비선형 커널의 사용은 로지스틱 회귀 및 다른 방법보다 SVM과 연결지어 널리 퍼져있다. 여기서는 다루지 않았지만 실제로 support vector regression이라고 하는 회귀가 있다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
