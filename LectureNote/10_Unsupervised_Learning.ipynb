{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISL 10. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저희는 지금까지 **Supervised Learning**에 대해서 다뤘습니다.  \n",
    "**Supervised Learning**은 response인 Y가 존재하며 이 Y에 대해 예측하는 것이 주 목적이었다면 **Unsupervised Learning**은 이와 다르게 response인 Y가 존재하지 않습니다.   \n",
    "따라서 **Unsupervised Learning**의 목적은 예측이 아닙니다.\n",
    "대신, **Unsupervised Learning**의 목적은 우리의 데이터 X들의 measurements에 대한 흥미로운 점이 있는지를 찾아내는 것입니다.  \n",
    "대표적인 예로는 Data내에서 Subgroup을 찾아내는 **Clustering** 등이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 The Challenge of Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 **Supervised Learning**보다 **Unsupervised Learning**이 더 어려울 수 있는데, 그 이유는 간단합니다.  \n",
    "앞서 언급했듯이 **Supervised Learning**과 다르게 **Unsupervised Learning**는 response Y가 존재하지 않기 때문입니다.  \n",
    "즉, 저희가 모델링을 통해 얻은 결과가 맞는 지 틀린 지, 또 얼마나 정확한 지를 제대로 알 수 없기 때문입니다.  \n",
    "하지만 의학, 추천 시스템등 **Unsupervised Learning**이 필요한 분야가 점차 늘고 있기 때문에 **Unsupervised Learning**에 대해서 공부할 필요가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1 What is Principal Components Analysis (PCA)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Components Analysis**란 **Principal Components(주성분)**들이 계산되고, 그러한 성분들이 data를 이해하는 과정에서 사용되는 일련의 Process를 의미합니다.  \n",
    "PCA는 Response는 전혀 신경쓰지 않고 오로지 X features들만 가지고 분석하는 것이기 때문에 **Unsupervised Learning**에 속합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are Principal Components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 PCA를 하는 이유는 우리가 가지고 있는 데이터를 손실은 최소화하면서도 원래보다 조금 더 낮은 차원에서 표현하기 위함입니다.  \n",
    "데이터를 조금 더 낮은 차원에서 표현할 수 있는 경우, 데이터 간의 관계를 좀 더 직관적으로 이해할 수 있고 시각화를 하기에도 용이하다는 이점이 있습니다.\n",
    "\n",
    "그리고 **Principal Components**는 PCA를 통해 발견된, 기존의 데이터의 정보를 담고 있는 다른 차원이라고 생각하면 됩니다. 그리고 이 PC들 중 중요한 몇 개를 고르는 작업을 통해 우리는 차원을 축소할 수 있습니다.\n",
    "\n",
    "그렇다면 PC는 어떻게 구할 수 있을까요?\n",
    "\n",
    "우선 각각의 PC들은 p feature들에 대한 Linear Combination이며 따라서 첫 번째 Principal Component 역시 Feature들에 대한 Normalized Linear Combination입니다.\n",
    "\n",
    "$Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + ... + \\phi_{p1}X_p$\n",
    "\n",
    "그리고 첫 번째 PC는 가장 큰 Variance를 가지고 있습니다. 또한 Normalized에 의해 $\\sum^p_{j=1}\\phi^2_{j1}=1$이라는 제약 조건이 생깁니다. 이 때 Normalized가 필요한 이유는 변수마다 스케일이 다르면 특정 방향으로 X의 분포가 찌그러져 나타날 수 있기 때문입니다.(variance가 제멋대로 나타날 수 있음)\n",
    "\n",
    "우리는 원소 $\\phi_{11}, ..., \\phi_{p1}$들을 first prinicpal component의 loadings라고 부릅니다. 그리고 이들을 묶은 $\\phi_1 = (\\phi_{11}, ..., \\phi_{p1})^T$를 loading vector라고 부릅니다.\n",
    "\n",
    "**PC Computation**\n",
    "\n",
    "그럼 n x p data set이 실제로 주어진 경우, 어떻게 첫 번째 PC를 계산할 수 있을까요?\n",
    "\n",
    "결론부터 말하면, 다음 식의 $z_{i1}$(우리는 이를 first pc의 score라고 부릅니다.)들의 sample variance를 가장 크게 만드는 Loading Vector를 구하면 됩니다.\n",
    "\n",
    "$z_{i1} = \\phi_{11}x_i1 + \\phi_{21}x_i2 + ... + \\phi_{p1}x_ip$\n",
    "\n",
    "sample variance를 가장 크게 만드는 이유는 그래야 data의 정보가 제일 잘 담기기 때문입니다. 그리고 이 때도 제약 조건은 $\\sum^p_{j=1}\\phi^2_{j1}=1$입니다. 따라서 이를 optimization problem 식으로 변환하여 적어보면\n",
    "\n",
    "$\\max_{\\phi_{11}, ..., \\phi_{p1}} \\Biggl\\{ \\frac{1}{n}\\sum^n_{i=1}(\\sum^p_{j=1}\\phi_{j1}x_{ij})^2 \\Biggr\\}$ subject to $\\sum^p_{j=1}\\phi^2_{j1}=1$\n",
    "\n",
    "이 때 sample variance를 maximize하는 것인데 왜 식은 $\\frac{1}{n}\\sum^n_{i=1}z^2_{i=1}$부분만 나와있는건가 할 수 있는데, 이는 앞서 우리가 스케일링을 하는 것이 전제 조건이므로, $\\frac{1}{n}\\sum^n_{i=1}x_{ij}=0$이 되므로, $z_{i1}$들의 평균도 0이 될 것이기 때문입니다.\n",
    "\n",
    "**Geometric Interpretation**\n",
    "\n",
    "이를 기하학적인 관점에서 해석해보면, Loading Vector $\\phi_1$은 data들이 가장 넓게 분포할 수 있는 방향입니다. 즉, n개의 데이터를 Loading Vector의 방향에 직교 투영하는 경우 그 퍼짐의 정도가 다른 vector들보다 제일 큰 경우가 First PC의 Loading Vector입니다.\n",
    "\n",
    "**After First Principal Component**\n",
    "\n",
    "첫 번째 PC를 구하고 나서 다른 PC를 구하는 방법은 위와 동일하지만 하나의 조건이 추가됩니다. 바로 $Z_2$는 $Z_1$과 Orthogonal, 즉, uncorrelated라는 조건입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC들을 모두 구한 후에는 이를 바탕으로 Plot을 그릴 수 있고 또 이를 통해 각각의 PC들이 대략 어떤 의미를 가지는 지 어느 정도 해석할 수 있습니다.\n",
    "\n",
    "다만, Plot은 PC 두 개를 한 쌍으로 하여 그릴 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Fig10.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 위의 plot에서 파란색 도시 이름들은 data를 나타내고 오렌지 색 Vector에 표시된 Murder 등은 Feature들을 나타냅니다.\n",
    "\n",
    "파란색 도시 이름들이 나타난 위치는 각각의 PC에서의 score에 따른 위치이며 Feature들의 Vector는 loading vector에 의해 나타나지게 됩니다.\n",
    "\n",
    "예를 들어, Rape의 First PC 상에서의 Loading이 0.54의 값을 가지고 Second PC 상에서의 Loading이 0.17을 가진다면, 위와 같이 나타나게 됩니다.\n",
    "\n",
    "위의 plot을 대략적으로 해석해보자면, First PC는 Rape, Assualt, Murder 즉, 강력범죄에 큰 loading을 주고 있다는 것을 알 수 있습니다. 따라서 First PC는 강력범죄에 관한 것이라고 해석할 수 있으며 반대로 Second PC는 앞서 세 개와 달리 UrbanPop 에 큰 loading을 주는 것으로 보아, 경범죄에 관한 것이라고 해석할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2  Another Interpretation of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 우리는 PC Loading Vector에 대해 데이터들이 가장 넓게 분포할 수 있는 방향이라고 설명했습니다. \n",
    "\n",
    "그런데 이는 다른 방식으로도 해석이 가능합니다.\n",
    "\n",
    "바로, 데이터들에 가장 가까운 Vector가 PC Loading Vector입니다.  \n",
    "특히 첫 번째 PC Loading Vector의 경우, n개의 관측치에 가장 가까운 p 차원 공간에서의 line이라는 특별한 성질을 가지고 있습니다. \n",
    "\n",
    "이 해석을 이용한다면, 첫 M 개의 PC score vectors와 loading vectors는 i번 째 관측치인 $x_{ij}$에 대해 Euclidean Distance 관점에서 가장 좋은 M 차원 접근을 제공합니다. 다시 말해, M 차원 상에서 본래의 데이터 $x_{ij}$들에 대한 가장 가까운 정보를 제공할 수 있습니다.\n",
    "\n",
    "이를 수식으로 쓰면\n",
    "\n",
    "$x_{ij} \\approx \\sum^M_{m=1}z_{im}\\phi_{jm}$\n",
    "\n",
    "그리고 M이 클수록 더 좋은 접근을 제공하며 M=min(n-1,p)인 경우, 정확히 $x_{ij} = \\sum^M_{m=1}z_{im}\\phi_{jm}$입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3 More on PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the Variables\n",
    "\n",
    "<br> PCA 수행 이전 변수들 **평균이 0이 되도록 스케일링** 과정을 거쳤습니다.(전제) 왜 할까요?? **데이터 측정 단위**가 다르기 때문이죠.\n",
    "<br> 스케일링을 하지 않은 변수에 PCA를 하면 (**측정 단위가 큰**) 특정 변수가 큰 분산을 가지므로 첫 번째 주성분로딩벡터(First PC Loading Vector)는 해당 변수에 대해 매우 큰 로딩을 갖게 됩니다. 스케일의 영향을 제거하기 위해 보통 PCA 수행 전 각 변수를 스케일링하여 **표준편차가 1**이 되도록 통일합니다.\n",
    "<br> 각 변수들이 같은 단위로 측정되는 상황에서는 스케일링 과정을 생략할 수도 있습니다.(다른 표준편차 값을 감안한다면)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Lab10-2-1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniqueness of the Principal Components\n",
    "\n",
    "<br> 각 PC Loading Vector는 *부호*가 다를 수는 있지만 **unique**합니다. \n",
    "<br> PC Loading Vectors 양쪽으로 확장되는 직선이라 방향에 영향이 없기 때문에 부호가 달라질 수 있는 것이고\n",
    "<br> Score Vector 역시 Z의 분산이 -Z의 분산과 같기 때문에 부호가 달라질 수 있습니다.\n",
    "<br> $x_{ij}$ 근사치 구할 때 PC Loading Vector와 Score Vector의 곱 표현이 나왔는데 둘 다 부호가 바뀌면 **곱은 그대로입니다.**\n",
    "\n",
    "#### The Proportion of Variance Explained(PVE)\n",
    "<br> PCA의 목적이 저차원으로 알짜배기 정보들을 표현하는 것인데 **전체 대비 데이터 손실**이 얼마나 발생할까요?? 즉 주성분들에 포함되지 않는 데이터의 분산은 얼마나 될까 하는 문제가 있습니다. **각 주성분에 의한 분산의 비율**을 알고자 합니다.\n",
    "<br> 평균이 0이 되도록 centralized 가정 하에 총 분산, m번째 PC에 의한 분산, m번째 PC의 분산 비율은 각각 다음과 같습니다. (10.6)~(10.8)\n",
    "<br> 여기서 모든 분산 비율들을 더하면 1이 되고 PC는 min(n-1,p)개만큼 있습니다. 특히 (10.7)의 식은 10.2.1절 첫 번째 **PC Loading Vector 최적화에서 살펴본 목적함수**와 같은 형태입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Lab10-2-2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Lab10-2-3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Lab10-2-4.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding How Many Principal Components to Use\n",
    "\n",
    "<br> 일반적으로 행렬 $X_{(n,p)}$는 min(n-1,p)개의 PC를 가집니다. 이들 모두를 사용하는 대신 몇 가지 중요한 PC들만을 추출해서 해석력을 높이고자 합니다.(시각화 등) 다만 여기서 \"몇 가지\"에 대해 객관적인 기준은 없다고 합니다. 회귀에서 R squared 값이 유의미하게 높아지는 수준까지 변수 추가하는 것과 비슷한 맥락 같습니다.\n",
    "\n",
    "<br> 각 성분의 PVE와 누적 PVE를 그린 plot을 참고하면, 각 성분의 PVE에서 특정 PC에 의한 PVE가 크게 감소하는 지점을 기준으로 합니다. 해당 지점을 Scree plot(좌측처럼 각 성분의 PVE plot)의 Elbow라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Lab10-2-5.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.4 Other Uses for Principal Components\n",
    "\n",
    "<br> 앞서 공부했던 회귀, 분류와 바로 다음의 클러스터링 등 통계 기법들에서 주어진 (n,p) 데이터 행렬을 이용하는 대신 열들이 M개의 PC만큼 주어진, (n,M) 행렬을 사용 -> 몇 개의 PC(주성분)에 집중하기에 noise가 적은 결과를 얻을 수 있다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.3 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터를 몇 가지 그룹별로(**sub-group**) 구분하는 기법. 이 때 그룹 내 데이터들은 상당히 유사하고 그룹 별로는 상이해야 할 겁니다. 얼마나 유사하고 상이하냐의 기준은 해당 데이터에 대한 지식을 요할 것입니다. 데이터를 기반으로 그 구조를 파악하기 위한 것으로 **Unsupervised Learning**의 하나입니다.\n",
    "<br> 예를 들어 고객들에 대한 여러 특성을 놓고(이를테면 직업, 연령, 성별 등등) 특정 제품에 대한 구매력을 기준으로 사람들의 sub-group을 포착해서 세분화할 수 있을 겁니다. \n",
    "<br> 이전에 배운 PCA와 Clustering 둘 다 데이터를 간단히 나타내려는 것인데 전자는 저차원 표현을 찾고자 하는 것이고, 후자는 **관측치들 내에서 일종의 소규모 그룹들**을 찾으려는 것이죠.\n",
    "<br> 책에서는 K-clustering과 Hierarchical clustering을 다룹니다.\n",
    "<br> K-clustering은 미리 그룹(cluster)의 갯수를 지정하는 것이고 Hierarchical clustering은 클러스터 개수를 알지 못하는 상태에서 관측치들에 대한 시각화는 dengrogram이라는 나무 모양이 됩니다. 이후의 절에서 자세히 살펴보도록 하죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 10.2.1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 원하는 클러스터의 수 K 지정\n",
    "\n",
    "2) 알고리즘이 각 관측치를 K 중 하나로 할당\n",
    "\n",
    "<br> 이를 만족하기 위해 K개의 클러스터는 겹치지 않고 각 관측치는 K 중 하나에만 속해야 합니다. 즉 K개의 클러스터는 Mutually exclusive이고 exhaustive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 데이터에 대해 할당된 라벨이 합당하려면 클러스터 내의 관측치들이 서로 비슷해야 할 겁니다(low within-cluster variation)\n",
    "<br> 클러스터 수인 K만큼 반복 계산해서 합산한 값을 최소화하도록.\n",
    "<br> 여기서 within-cluster variation을 일반적으로 유클리드 거리 제곱을 적용하여 정의합니다.\n",
    "<br> 분모에 $ C_{k}$ 는 k번째 클러스터 내 관측치의 수, 아래 식은 k번째 클러스터 내의 모든 쌍에 대해서 유클리드 거리 제곱을 계산(p개의 변수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 10.2.2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 위 식을 모든 case에 대해 계산하기는 계산량이 많기 때문에 대안책을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 10.2.3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Lab10.2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 관측치를 1~K에 랜덤으로 할당한 다음\n",
    "<br> 클러스터별로 무게중심을 계산하여(그룹 내 관측치들에 대한 p 변수 평균들의 벡터)\n",
    "<br> 각 관측치를 가장 가까운 클러스터에 할당(유클리드 거리로 판단)\n",
    "\n",
    "항등식에서 $ \\bar{x}_{kj} $ 는 k번째 클러스터 내의 변수 j에 대한 평균"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 10.2.4.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "국소적인 영역에서 최소값을 찾는 방법이기에 초기 랜덤값을 다르게 주어 반복 실행한 다음 목적 값이 최소인 것을 선택.\n",
    "<br> 한편 초기에 K를 설정하는 것에 대한 고려사항 등은 후절에서 논의하기로 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering의 단점은 cluster의 개수 K를 임의로 정해야 함에 있다. 반면에 Hierarchical clustering은 cluster의 개수를 정할 필요 없으며 observation을 dendrogram을 통해 tree 형태로 나타낼 수 있다는 장점이 있다. 이번 장은 Hierarchical clustering 중에서 가장 일반적인 bottom-up 또는 agglomerative clustering을 살펴 볼 예정이다. bottom-up인 이유는 observation이 나뭇잎부터 시작해서 가지로 그룹되며 최종적으로 하나의 줄기으로 연결되기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting a Dendrogram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 10.8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting**\n",
    "- True class label: 각 45 observation은 3개의 class 중 (초록, 보라, 주황) 하나에 속한다. (Figure 10.8)\n",
    "- Hierarchical clustering with complete linkage를 이용해 three-class model을 만들었으며 이를 해석하고자 한다. (Figure 10.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 10.9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendrogram의 height이 0에서 각 나뭇잎은 한 observation에 대응한다. 높이가 증가함에 따라 서로 유사한 나뭇잎끼리 가지로 그룹화 된다. 높이가 더 증가하면 나뭇잎과 그룹, 그룹과 그룹간 결합도 가능하다. 따라서 dendrogram의 height은 observation간 유사성을 나타낸다. 만약 두 observation이 유사한 경우, 나무 높이가 낮은 구간에서 결합할 것이고, 반대로 두 observation이 상이한 경우 높은 구간에서 첫 결합이 발생할 것이다. 주의할 점은 dendrogram의 x축이 아닌 y축이 유사성을 나타낸다. 아래 Figure 10.10의 왼쪽 dendrogram을 살펴보자. Observation 1과 6은 높이가 낮은 구간에서 결합하기 때문에 유사한 observation이다. 마찬가지로 observation 9와 2의 유사성은 observation 9와 observation group \\{8,5,7\\}의 유사성과 동일하다. \n",
    "<br>\n",
    "<br>\n",
    "Dendrogram을 이용해 clustering을 하는 방법은 Figure 10.9의 중간 및 우측 패널에 나와있다. 중간 패널의 경우 높이 9에서 dendrogram에 수평선을 긋고, 여기서 잘려나온 가지가 두개이다. 각 가지 아래 속한 observation의 집합이 바로 **cluster**이다. 우측 패널의 경우 높이 5에서 dendrogram에 수평선을 긋고, 여기서 잘려나온 가지가 세개이기 때문에 cluster가 3개이다. \n",
    "<br><br>\n",
    "K를 하나의 값으로 고정해야 하는 K-means clustering과 달리 hierarchical clustering의 장점은 dendrogram 하나로 cluster의 개수를 다양하게 지정할 수 있다. 물론 어느 높이에서 dendrogram을 자를지 (cluster의 개수 정하기)는 분석자 마음이다. \n",
    "<br><br>\n",
    "다만 한 높이에서 dendrogram을 잘라서 생선한 cluster가 더 높은 높이에서 자른 dendrogram에서 생성된 cluster에 항상 nested되어 있다는 것이 단점이다. 예를 들어 남녀 및 세 국가로 균등하게 속한 50명 개인에 대한 observation이 있다고 가정하자. 만약 2개의 cluster을 이용하면 남녀, 3개의 cluster을 이용하면 국가로 clustering하는 것이 가장 이상적일 것이다. 이때 true cluster들은 nested 되어있지 않지만 hierarchical clustering을 이용하면 더 높은 높이에서 dendrogram을 잘라서 생성한 두 개의 cluster은 더 낮은 높이에서 dendrogram을 잘라서 생성한 세 개의 cluster을 필연적으로 nest할 것이기 대문에 K-means clustering보다 성능이 좋지 않을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 10.10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Hierarchical Clustering Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알고리즘 시각화\n",
    "<br> \n",
    "<img src=\"pic/Figure 10.11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Algorithm 10.2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**linkage**: dissimilarity measure between pair of groups of observations\n",
    "- complete\n",
    "- average\n",
    "- single \n",
    "- centroid: inversion이 발생할 수 있기 때문에 시각화 및 해석이 어려움. \n",
    "    - inversion: 두 cluster가 결합할때 어느 한 cluster의 dendrogram 높이 보다 더 낮은 곳에서 결합하는 경우\n",
    "- average와 complete linkage가 가장 많이 쓰인다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 10.12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choice of Dissimilarity Measure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Euclidean distance**\n",
    "    - ex) 쇼핑 내역 data로 쇼핑객의 clustering: 인터넷 쇼핑 사용량이 적은 쇼핑객들끼리 clustering\n",
    "2. **Correlation-based distance**: 두 observation 간 상관관계가 높을 수록 더 높은 유사성을 가지는 measure\n",
    "    - ex) 쇼핑 내역 data로 쇼핑객의 clustering: 유사한 소비 선호도를 가진 쇼핑객들끼리 clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/Figure 10.13.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling의 필요성\n",
    "- 쇼핑 예시의 경우 양말의 구매량이 컴퓨터의 구매량 보다 더 높기 때문에 쇼핑객 간 차이를 설명하는 데 더 큰 작용을 하며 clustering이 될 것이다. 이를 보완하고 단위도 정규화 하기 위해 scaling을 사용할 수 있다. \n",
    "<br>\n",
    "<img src=\"pic/Figure 10.14.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3 Practical Issues in Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Small Decisions with Big Consequences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering하기 이전 다음과 같은 결정을 해야하는데, 결정에 따라 매우 다른 결과과 나올 수 있다. \n",
    "- hierarchical clustering의 경우\n",
    "    - 어떤 dissimiliarity measure이 사용돼야 하는가?\n",
    "    - 어떤 linkage를 사용해야 하는가?\n",
    "    - cluster를 얻기 위해 어디에서 dendrogram을 잘라야 하는가? \n",
    "- K-means clustering의 경우 몇 개의 cluster을 사용해야 하는가? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating the Clusters Obtained**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering을 진행한 후 만들어낸 cluster들이 true subgroups를 나타내는지 또는 단순히 noise를 cluster 했는지 알아야 한다. 물론 합의된 방법은 없지만 각 cluster에 대해 p-value를 구해 cluster가 subgroup을 나타내는지 또는 우연히 cluster된 것인지 판별할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Considerations in Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 observation은 어떤 subgroup에 속해 있을 수 있지만, 어떤 observation 같은 경우 모든 observation가 다를 수 있다. 하지만 K-means clustering과 hierarchical clustering은 모든 observation을 어떤 cluster에 배정하기 때문에 이 처럼 outlier가 있는 경우 해당 cluster가 왜곡될 가능성이 있다. 또한 clustering은 data의 perturbation에 대해 robust하지 않는다. n개의 observation 중 일부분을 무작위로 제거하고 clustering을 진행하면 n개를 모두 사용해서 나온 cluster과 매우 다른 cluster이 나올 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Tempered Approach to Interpreting the Results of Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 parameter를 사용해 clustering을 여러번 진행해 공통적인 패턴을 찾고, data의 부분집합에 대해 여러번 clustering하여 non-robustness를 보완하는 것이 중요하다. Clustering 결과는 hypothesis나 research question을 시작하기 이전 data 구조에 대한 이해를 위해 참고로용으로 사용하는 것이 좋다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
