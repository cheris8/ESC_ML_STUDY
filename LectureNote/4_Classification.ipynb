{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISL STUDY CH4. CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Regression VS Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression과 Classification 차이\n",
    "\n",
    "\n",
    " * Regression: 종속변수가 수치형(Quantitative, Numerical)  \n",
    "   ex)나이, 몸무게, 소득, 주가 등\n",
    "  \n",
    "  \n",
    " * Classification: 종속변수가 범주형(Qualitative, Categorical)  \n",
    "   ex)성별, 브랜드, 감염 여부 등\n",
    "   \n",
    "   \n",
    " * Classification에서 $y_0$의 추정은 곧 $y_0$를 분류(Classification)하는 것.\n",
    " \n",
    " \n",
    " * 대표적인 Classifier:  \n",
    "   Bayes Classifier  \n",
    "   K-Nearest Neighbors  \n",
    "   Logistic Regression  \n",
    "   Linear Discriminant Analysis  \n",
    "   etc.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Not Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Linear Regression의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  만약 종속변수가 범주형인 경우, 각 항목에 해당하는 값을 어떻게 설정하느냐에 따라 결과가 달라지는 문제가 있다. \n",
    "  \n",
    "  예를 들어  \n",
    "  $Y=\\begin{cases} 1\\; \\text{if A} \\\\ 2\\; \\text{if B} \\\\ 3\\; \\text{if C} \\end{cases}$  \n",
    "  \n",
    "  이렇게 설정한 거랑  \n",
    "  \n",
    "  $Y=\\begin{cases} 1\\; \\text{if C} \\\\ 2\\; \\text{if B} \\\\ 3\\; \\text{if A} \\end{cases}$  \n",
    "  \n",
    "  이렇게 설정한 것은 결과값이 다르게 나온다.\n",
    "  \n",
    "#### Binary Case\n",
    "\n",
    "  Y의 범주가 2개인 Binary Case의 경우에 $Y\\in \\{0, 1\\}$으로 나타내어 추정값인 $\\hat{y_0}$가 0.5를 넘는 경우 Y=1로 분류하고 0.5미만인 경우 0으로 분류할 수 있다. \n",
    "   \n",
    "   이 때 $\\hat{Y}=X^T\\beta$는 조건부 확률인 P(Y=1|X)으로 해석 가능.\n",
    "   \n",
    "   그러나 문제는 OLS로 Fitting할 시, P(Y=1|X)값이 0보다 작거나 1보다 큰 경우가 발생할 수 있다. 따라서 이러한 문제를 해결한 것이 바로 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Linear Regression과 Logistic Regression의 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Logi.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - The Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 우리가 가진 데이터 X로 $p(X)=p(Y=1|X)$를 fitting 해보자.\n",
    " \n",
    " OLS를 그대로 사용하는 경우 $p(X)=\\beta_0+\\beta_1X$ \n",
    " \n",
    " 여기서 우변의 함수 형태를 다음과 같이 바꿔보면 $p(X)$가 [0,1]구간 안에 포함된다!\n",
    " \n",
    " $p(X)=\\frac{exp(\\beta_0+\\beta_1X)}{1+exp(\\beta_0+\\beta_1X)}$\n",
    " \n",
    " 우변이 $\\beta_0+\\beta_1X$형태가 되도록 식을 다시 정리하면,\n",
    " \n",
    " $log(\\frac{p(X)}{1-p(X)})=\\beta_0+\\beta_1X$\n",
    " \n",
    " 여기서 좌변에 해당하는 $log(\\frac{p(X)}{1-p(X)})$를 우리는 Y=1일 확률의 log-odds 또는 logit이라 한다.\n",
    " \n",
    " 따라서 Logistic Regression은 Y=1일 확률의 logit에 대한 선형 모델이라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - The Logistic Model에서 $\\beta$의 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  $\\frac{p(X)}{1-p(X)}=exp(\\beta_0+\\beta_1X)$의 관계식에서 알 수 있듯이 \n",
    "  \n",
    "  X의 한 단위 변화는 \"log odds\"를 $\\beta_1$만큼 변화시킨다. \n",
    "  \n",
    "  즉, X가 한 단위 변화한다고 해서 p(X)의 값이 $\\beta_1$값만큼 증가, 또는 감소하는 관계가 아니다. \n",
    "  \n",
    "  X값의 변화에 따른 p(X)값의 변화는 1) 회귀계수의 크기와 방향 2) 현재 X의 수준에 따라 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Maximum Likelihood Estimation of $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Logistic Regression 에서 가정한 개별 y의 Sampling Density의 식은 다음과 같다.\n",
    "  \n",
    "  * Sampling Density of $y|X = \\begin{cases} p(y=1|X)=p(X)=\\frac{exp(\\beta_0+\\beta_1X)}{1+exp(\\beta_0+\\beta_1X)} \\\\ p(y=0|X)=1-p(X)=1-\\frac{exp(\\beta_0+\\beta_1X)}{1+exp(\\beta_0+\\beta_1X)} \\end{cases}$ \n",
    "  \n",
    "  N개의 $(x_i,y_i)$ iid sample 데이터에 대한 Joint Sampling Density는 다음과 같다(이 때 Y는 벡터). \n",
    "  <br>\n",
    "  \n",
    "  * Joint Sampling Density of $p(Y|X)=\\prod_{i|y_i=1}p(X_i)\\prod_{j|j=0}(1-p(X_j))$\n",
    "  \n",
    "  얘를 MLE의 측면에서 해석하면, 데이터가 주어졌을 때 $\\beta$의 식, Joint Likelihood로 볼 수 있다. 이 식을 최대화하는 $\\beta$를 추정하는 것이 MLE\n",
    "  \n",
    "  * Joint Likelihood function $L(\\beta_0,\\beta_1|Y,X)=\\prod_{i|y_i=1}p(X_i)\\prod_{j|j=0}(1-p(X_j))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Response with more than 2 Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 위에서는 종속변수가 2개인 경우를 살펴보았다. 그러면 세 개인 경우는 어떨까??\n",
    "  \n",
    "  $Y \\in \\{A,B,C \\}$인 경우를 생각해보자.\n",
    "  \n",
    "  * Sampling Density of $y|X = \\begin{cases} p(y=A|X)=\\frac{exp(X^T\\beta_A)}{1+exp(X^T\\beta_A)} \\\\ p(y=B|X)=\\frac{exp(X^T\\beta_B)}{1+exp(X^T\\beta_B)} \\\\\n",
    "p(y=C|X)=1-p(y=A|X)-p(y=B|X) \\end{cases}$  \n",
    "   <br>\n",
    "\n",
    "  * Likelihood $L(\\beta_A,\\beta_B)=\\prod p(A|X)\\prod p(B|X)\\prod p(C|X)$\n",
    "  <br>\n",
    "  \n",
    "  * 이 경우 범주가 추가될 때마다 추정해야 하는 모수의 개수가 p개 만큼 추가된다.\n",
    "  <br>\n",
    "  \n",
    "  * 그러나 범주가 2개보다 많은 경우에는 이 방법보다는 대부분 다른 방법을 쓴다. 대표적인 겻이 다음에 나오는 Linear Discriminant Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - LDA Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Classification Parametric Methods 근본 원리는 동일하다. 결국 Bayes Classifier를 추정하여 가장 높은 값을 가지는 애를 찾고 그에 해당되는 class를 확인하는 것.\n",
    "  \n",
    "  * Bayes Classifier: $p_k(X)=p(Y=k|X=x)$\n",
    "  \n",
    "  그런데 우리가 하는 일은 Bayes Classifier의 절대적인 크기보다는 그 순서이다. 따라서 Bayes Classifier를 Monotone transformation 한 뒤 제일 큰 애를 골라내도 똑같음.\n",
    "  <br>\n",
    "  \n",
    "  * Discriminant: $\\delta_k(X)=f(p(Y=k|X=x))$ \n",
    "  \n",
    "  ** LDA도 Discriminant의 선형함수라고 할 수 있는데 다만 Logistic과의 차이라면 Logistic은 추정 방식에서 MLE를 사용, LDA는 Bayes Rule을 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Estimating Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Bayes Classifier $p(Y=k|X=x)=\\frac{p(Y=k\\; and\\; X=x)}{p(X=x)}=\\frac{p(Y=k)p(X=x|Y=k)}{\\sum_kp(Y=k)p(X=x|Y=k)}=\\frac{\\pi_kf_k(X)}{\\sum_{i=1}^K\\pi_kf_i(X)}$\n",
    "  <br>\n",
    "  \n",
    "  * $\\pi_k$:Prior probability of each class k $\\rightarrow$ 보통 전체 N개의 관측치 중 class k인 관측치 개수의 비율로 쉽게 구함.\n",
    "  <br>\n",
    "  \n",
    "  * $p_k(X)=p(k|X)$: Predictor X가 주어졌을 때 관측치가 class k에 속할 확률. 우리가 구해야 하는 target.\n",
    "  <br>\n",
    "  \n",
    "  * $f_k(X)=p(X|k)$: Likelihood of X in class k. 우리가 가정하는 부분. 이것에 대한 가정에 따라 LDA와 QDA로 나뉜다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - LDA: p=1 One Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  LDA에서는 X|y=k 가 Normal Distribution을 따른다고 가정한다.\n",
    "  \n",
    "  * $f_k(X)=\\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}exp(-\\frac{(x-\\mu_k)^2}{2\\sigma_k^2})$\n",
    "  \n",
    "#### LDA의 가정\n",
    "  \n",
    "  * Different Mean & Shared Variance  \n",
    "  \n",
    "   각 클래스마다 X의 평균은 다르지만, 모든 클래스에서의 X의 분산은 동일하다. 즉, 클래스마다 $\\sigma_k^2$를 따로 구할 필요가 없음. 그냥 $\\sigma^2$으로 통일\n",
    "   \n",
    "   $\\rightarrow$ $f_k(X)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu_k)^2}{2\\sigma^2})$\n",
    "   \n",
    "   이를 Bayes Classifier $p(Y=k|X)$에 대입하면\n",
    "   \n",
    "   $\\rightarrow$ $p(K|X)=\\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu_k)^2}{2\\sigma^2})}{\\sum_l\\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu_l)^2}{2\\sigma^2})}$\n",
    "   \n",
    "   보다시피 식이 많이 복잡해보인다. 그러니까 좀 더 쉽게 간단하게 단조변환해보자.\n",
    "   \n",
    "   * Population Linear Discriminant: $\\delta_k(x)=x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k)$\n",
    "   \n",
    "#### Parameter 추정\n",
    "   \n",
    "   그럼 이제 위 식에서 모수 $\\mu_k$, $\\sigma^2$, $\\pi_k$를 추정해보자.\n",
    "   \n",
    "   * $\\hat{\\mu_k}$: class k인 관측치 x의 평균\n",
    "   <br>\n",
    "   \n",
    "   * $\\hat{\\sigma^2}$: 각 클래스 내에서의 편차 제곱을 모두 더해 N-k로 나눈 것\n",
    "   <br>\n",
    "   \n",
    "   * $\\hat{\\pi_k}$: 전체 관측치 중 class k의 비율  **만약 모든 클래스 별 관측치의 개수가 동일한 경우, $log(\\hat{\\pi_k})$는 의미가 없음. 있으나 마나..\n",
    "\n",
    "   $\\rightarrow$ Sample Linear Discriminant: $\\hat{\\delta_k(x)}=x\\frac{\\hat{\\mu_k}}{\\hat{\\sigma^2}}-\\frac{\\hat{\\mu_k^2}}{2\\hat{\\sigma^2}}+\\log(\\hat{\\pi_k})$\n",
    "   \n",
    "#### 결론\n",
    "   \n",
    "   어떠한 관측치 $x_0$에 대해 각각의 class k에 대한 $\\hat{\\delta_k(x)}$를 모두 계산해 그 값이 가장 큰 class k로 관측치 분류\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - LDA: p>1 Multiple Predictor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  p>1의 경우 X|y=k ~ Class specific $MVN(\\mathcal{M}_k,\\sum)$\n",
    "  \n",
    "  * $f_k(X)=\\frac{1}{(2\\pi^2)^{p\\, /\\, 2}|\\sum|^2} exp(-\\frac{(X-\\mathcal{M}_k)^T\\sum^{-1}(X-\\mathcal{M}_k)}{2})$\n",
    "  <br>\n",
    "  \n",
    "  * Population Linear Discriminant: $\\delta_k(x)=x^T\\sum^{-1}\\mathcal{M}_k-\\frac{1}{2}\\mathcal{M}_k^T\\sum^{-1}\\mathcal{M}+\\log(\\pi_k)$\n",
    "  \n",
    "#### Parameter 추정\n",
    " \n",
    "  * $\\hat{\\mathcal{M}_k}$: class k에서 각 $x_i$ 관측치의 변수별 평균\n",
    "  <br>\n",
    "  \n",
    "  * $\\hat{\\sum_{i,j}^2}$: 각 클래스에서의 i변수와 j변수의 편차 곱의 합을 모두 더해 N-k로 나눈 것\n",
    "  <br>\n",
    "  \n",
    "  * $\\hat{\\pi_k}$: 전체 관측치 중 class k의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Bayesian Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  set of $X=(x_1, x_2, ...,x_p)$ where $\\delta_k(X)=\\delta_l(X)\\;$ for $k\\ne l$\n",
    "  \n",
    "  $\\rightarrow$ 즉, 서로 다른 class에 대해 같은 delta헷 값이 나온다면 걔는 경계선이 된다.\n",
    "  \n",
    "  <img src='pic/BDB.png' align='left'></img>\n",
    "  <img src='pic/BDB2.png' align='left'></img>\n",
    "  <br>\n",
    "  <br>\n",
    "  <br>\n",
    "  <br>\n",
    "  \n",
    "  $\\rightarrow$ 세가지 클래스에 대한 Bayesian Decision Boundary.   \n",
    "  타원은 각 클래스가 95%의 확률로 나타날 구간을 의미\n",
    "  \n",
    "  $\\rightarrow$ 오른쪽 그림은 새로운 data들이 나타났을 때 점선에서 실선으로 Boundary가 이동했음을 보여줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - LDA: Training Error Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting Problem\n",
    "  \n",
    "   LDA에서의 Training Error Rate는 대부분 Test Error Rate보다 낮다.  \n",
    "  $\\rightarrow$ Training Data로 Bayes Classifier를 추정하기 때문  \n",
    "  $\\rightarrow$ 이 때문에 만일 모수의 개수와 관측치의 비율인 $\\frac{p}{N}$이 크면 Overfitting 문제가 발생할 수 있다.\n",
    "  \n",
    "#### Null Classifier\n",
    "  \n",
    "  Y=0 or 1에서 그냥 모든 관측치를 Y=1로 분류하는 경우이다.  \n",
    "  $\\rightarrow$ 이 때의 Error Rate은 전체 관측치에서 0의 비율  \n",
    "  $\\rightarrow$ 만약 모델의 Training Error Rate가 이거보다 낮다면 안 쓰는게 낫다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - LDA: Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix  \n",
    "  \n",
    "  타겟의 원래 클래스와 모형이 예측한 클래스가 일치하는지는 갯수로 센 결과를 표나 나타낸 것이다.   \n",
    "  정답 클래스는 행(row)으로 예측한 클래스는 열(column)로 나타낸다.\n",
    " \n",
    "  <img src='pic/CM.png' align='left' ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - LDA: Threshold and ROC, AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  오류에는 두 가지 종류가 있다. 1)False Positive(Type I Error), 2)False Negative(Type II Error)\n",
    "  \n",
    "  상황에 따라서 두 가지 오류 중 한 가지가 더 중요한 경우가 있다. 예를 들어, 파산 예측에서는 실제 파산할 사람인데(실제 True) 파산하지 않는다고 예측(분류 False)하는 경우가 더 큰 문제이다. 즉 False Negative를 줄이는 게 더 중요.\n",
    "  \n",
    "  그런데 문제는 Bayes Classifier에서는 두 종류의 오류를 동등하게 취급. 따라서 두 가지의 오류 중 하나를 줄이고 싶다면 Threshold를 조정해주어야 한다.\n",
    "  \n",
    "#### Threshold\n",
    "  \n",
    "  1. Threshold > 0.5 조절  \n",
    "  \n",
    "   이 경우는 확률이 0.5보다 높아야 1로 분류하는 것이므로 1로 분류하는 것이 적어지는 상황. 따라서 True를 1이라고 뒀을 때, False Positive가 줄어들게 되고 False Negative는 높아지게 된다. 즉, 0.5보다 높이면 Type I Error는 줄고 Type II Error는 늘어난다.\n",
    "   \n",
    "   결론: Type I Error 감소 II 증가\n",
    "   <br>\n",
    "   \n",
    "  2. Threshold < 0.5 조절\n",
    "  \n",
    "   위와 반대의 상황 따라서 Type II Error 감소 I 증가\n",
    "   \n",
    "   앞서 파산의 예시에서는 2번을 선택해야 함.\n",
    "   \n",
    "  \n",
    "  <img src='pic/Threshold.png' align='left'></img> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC & AUC\n",
    "  \n",
    "  이를 ROC 커브로 그려보면\n",
    "  \n",
    "  <img src='pic/ROC.png' align='left'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - QDA 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  QDA는 LDA랑 거의 동일하지만 차이가 있다면 분산의 가정에서 차이가 난다. LDA에서는 class 별 분산이 같다고 가정하였지만 QDA에서는 다르다고 본다.\n",
    "  \n",
    "  * Population Discriminant:  \n",
    "  \n",
    "  $\\quad \\delta_k(x)=-\\frac{1}{2}(x-\\mu_k)^T\\sum_{k}^{-1}(x-\\mu_k)-\\frac{1}{2}\\log|\\sum_k|+\\log\\pi_k \\\\ \\quad \\qquad= -\\frac{1}{2}x^T\\sum_{k}^{-1}x+x^T\\sum_{k}^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T\\sum_k^{-1}\\mu_k-\\frac{1}{2}\\log|\\sum_k|+\\log\\pi_k $\n",
    "  \n",
    "  ** 보면 Discriminant가 X에 대한 Quadratic 함수이다. 이 때문에 QDA라 부름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Var-Bias Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * QDA를 사용하는 경우 Covariance Matrix에서 추정해야 하는 Parameter의 개수가 무려 $K\\times \\frac{p(p+1)}{2}$개이다.\n",
    "  \n",
    "  $\\rightarrow$ 즉, QDA는 LDA에 비해 훨씬 유연하며, 따라서 training 데이터가 적은 경우 분산이 굉장히 클 수 있다.\n",
    "  \n",
    "  $\\rightarrow$ 대신 실제로 Discriminant가 비선형인 경우, QDA를 사용함으로써 모델의 Bias를 줄일 수 있다.\n",
    "  \n",
    "  $\\rightarrow$ 즉, Variance는 올라가는 대신, Bias는 줄어듦.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * $p(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i\\in N_0}I(y_i=j)$\n",
    "  \n",
    "  $\\rightarrow$ 어떤 점 $x_0$에서 '가장 가까운' K개의 점들의 집합을 $N_0$이라 함\n",
    "  \n",
    "  $\\rightarrow$ 그 중 class j의 총 개수: $\\sum_{i\\in N_0}I(y_i=j)$\n",
    "  \n",
    "  $\\rightarrow$ 이렇게 구한 추정 조건부 분포에서 가장 확률이 높은 class가 선택된다.\n",
    "  \n",
    "  $\\rightarrow$ K의 값은 우리가 임의로 설정하는데, 더 많은 점을 볼수록 더욱 경직적인 Decision Boundary\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src='pic/KNN1.png' align='left'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src='pic/KNN2.png' align='left'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. A Comparison of Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Classification 방법 별 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logit vs LDA\n",
    "  \n",
    "  둘 다 Bayes Classifier의 선형 모수 추정이라는 점에서 비슷하지만 가정이 다르다.\n",
    "  \n",
    "  $\\rightarrow$ LDA: 각 class에서 X의 분포가 정규 분포임을 가정한다. 이러한 가정이 맞을 경우 LDA의 성능이 더 좋을 수 있지만, 아닌 경우 가정이 없는 Logit이 더 좋을 수 있다.\n",
    "  \n",
    "#### KNN\n",
    "  \n",
    "  KNN은 특정한 모수가 없는 Bayes Classifier의 비모수 추정이다. 이 때 주변의 몇 개의 점을 볼 것인가에 따라 굉장히 유연할 수도 경직적일 수도 있다.(많이 볼수록 경직적)\n",
    "  \n",
    "#### QDA\n",
    "  \n",
    "  QDA는 모수의 개수를 추가함으로써 LDA, Logit과 KNN의 사이에 있다고 할 수 있다.  \n",
    "  실제 Decision Boundary가 비선형인 경우, 적은 수의 표본으로도 QDA가 좋은 성능을 낼 수 있다.\n",
    "  <br>\n",
    "  \n",
    "   $\\rightarrow$ 결론: 어떤 상황인지를 보고 그 상황에 알맞은 방법을 쓰자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/comp.png'></img>\n",
    " \n",
    " \n",
    "   $\\rightarrow$ Linear Decision Boundary인 경우이다. 보면 Logit이나 LDA가 성능이 좋고 QDA는 별로임을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/comp2.png'></img>\n",
    "\n",
    "  $\\rightarrow$ Non-Linear의 경우 Logit이나 LDA는 성능이 별로고 QDA가 좋음을 알 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
