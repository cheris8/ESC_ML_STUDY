{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISL 07: Moving Beyond Linearity\n",
    "<br>\n",
    "우리는 지금까지 linear model들에 대해 다뤄왔다. linear model은 단순해서 이해하기 쉬우나, 예측력 측면에서 한계가 있다. \n",
    "<br>선형모델은 선형성 가정을 하는데, 현실과 잘 맞지 않는 경우가 있기 때문이다. 이에 6장에서 Ridge, Lasso, PCR 등 선형모델을 개선시키는 방법을 다뤘다. \n",
    "<br> 7장에서는 우리가 충분히 이해할 수 있으면서도 선형가정을 완화시킬 수 있는 방법을 알아보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Polynomial regression: 기존에 다차항을 추가하여 non-linear data에 적합할 수 있게 선형모델을 확장**\n",
    "<br><br>\n",
    "- **Step function: 변수를 K개의 구간으로 나누어 질적변수를 만들어낸다.이는 piecewise constant function에 적합하는 효과를 갖는다.** <br>(Piecewise 함수: 여러 부분-함수로 정의되는 함수. 각 부분-함수는 주 함수의 도메인의 특정 간격, 부분-도메인에 적용된다.)\n",
    "<br><br>\n",
    "- **Regression splines: 전체 X를 K개 구간으로 나누고, 각각의 부분 내에서 다항적합을 하는, 위의 두 방식의 확장이라고 볼 수 있겠다. 이때 다항적합은 양 옆 구간의 다항함수와 매끄럽게 연결되어야 한다. 잘 나눌 경우 regression spline은 유연한 적합을 가능케 한다.** \n",
    "<br><br>\n",
    "- **Smoothing splines: regression splines과 비슷하지만, smooth penalty를 포함한 SSE를 최소화하는 방식으로 적합한다.**\n",
    "<br><br>\n",
    "- **Local regresson: spline방식과 비슷하지만, 각 구간이 겹칠수 있다. 이 방식은 적합을 더 유연하게 한다.**\n",
    "<br><br>\n",
    "- **Generalized additive model: 위 방식들을 여러 개 예측변수에 적용 가능하게 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Polynomial Regression\n",
    "<br>\n",
    "non linear한 데이터를 적합하는 기본적인 방법이 바로 다항식을 추가하는 것이다. 다항회귀식은 다음과 같다. \n",
    "<br><br>\n",
    "$$y_i = β_0 + β_1x_i + β_2x_i^2 + β_3x_i^3 +...+β_dx^d+ ϵ_i$$\n",
    "<br> \n",
    "이는 새로운 예측변수로써 다차항을 넣은 것과 같다. (엄밀히 말하면 d차항들에 대한 선형모델)\n",
    "<br> 일반적으로 3,4차항 정도까지만 쓰는데, 그보다 높은 차수는 지나치게 flexible하고 모양이 이상해지기 때문이다. \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36205902-62ddbbda-1188-11e8-9af7-b41190172a16.PNG\" style=\"zoom:77%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽 그림의 파란 선은 Wage~Age의 자료에 대해 4차적합을 한 것 \n",
    "점선은 예측값의 분산을 통해 구한 신뢰띄이다. 예측값의 분산은 단순회귀와 동일하게 구할 수 있다. 다음과 같은 식에서 \n",
    "<br>\n",
    "<img src=\"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/7.12.JPG?raw=true\" style=\"zoom:37%;\" />\n",
    "<br>\n",
    "각 $β_j$의 variance와 각 쌍간의 covariance를 각 점에 대해 구하고 선으로 이은 것이다. \n",
    "<br>\n",
    "<br>\n",
    "그림을 보면 wage가 250을 기준으로 2개의 다른 집단, 즉 고소득층과 저소득층으로 나뉜다.\n",
    "<br> 이 경우 역시나 다음과 같이 다항로지스틱 적합을 할 수도 있다.(로지스틱의 경우 확률을 반환) \n",
    "<br>\n",
    "<img src=\"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/7.13.JPG?raw=true\" style=\"zoom:47%;\" />\n",
    "<br>위의 오른쪽 그림에 위 식을 통해 적합된 확률이 나와있다. 고소득층에 대한 데이터가 79개로 적고, 분산이 높아서 결과적으로 95%신뢰구간이 넓게 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다항함수를 사용하는 것은 예측변수 전 구간에 **non-linear**형태를 부여하는 것과 같다.\n",
    "<br>step function에서는 전체 X를 몇개의 구간으로 나눈다. 그리고, 그 구간마다 일정한 상수를 부여한다. \n",
    "<br>이는 연속형 반응변수 Y를 몇개의 범주형 변수로 바꿔서 부여하는것과 같다. 식으로 나타내면 다음과 같다.\n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486473-6c31826e-1716-11e8-8267-ecf2eef8dcd6.PNG\" style=\"zoom:77%;\" />\n",
    "<br>전체 X를 K개 범주로 나누고, 각 범주에 대해 상수를 부여했다. \n",
    "<br> 범주에 들어가면 1이고 나머지는 0이된다는 점에서 dummy variable과 비슷하게 이해하면 쉽다. 식의 계수들은 least square로 적합하여 구하면 된다. \n",
    "<br><br>\n",
    "다음과 같은 식으로 logistic regressoin model에 적합할 수도 있다.\n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486467-6ba3d734-1716-11e8-9607-65ce3cf44892.PNG\" style=\"zoom:77%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Step Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486470-6bdab43e-1716-11e8-9d22-1b5b4a728843.PNG\" style=\"zoom:77%;\" />\n",
    "그러나, 예측 변수에 자연스러운 중단점이 없으면 step function은 작업을 놓칠 수 있다. \n",
    "<br>예를 들어, 왼쪽 그림에서 첫 번째 구간은 연령에 따른 임금 상승 추세를 분명히 놓치고 있다.\n",
    "<br> 즉, 구간을 어떻게 나누느냐에 따라 제대로 작동하지 못할 수 있는 것이다. (그럼에도 불구, 이 방식은 생물 통계학 및 역학에서 매우 인기 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 7.3 Basis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사실, 앞서 다룬 polynomial과 piecewise-constant regression은 basis function의 특수한 경우이다.\n",
    "<br>basis function는 다음과 같은 식으로 나타낼 수 있다.\n",
    "<br><br>\n",
    "$$ y_i = β_0 + β_1b_1(x_i)+β_2b_2(x_i)+β_3b_3(x_i)+...+ β_Kb_K(x_i)+ϵi. $$\n",
    "<br>\n",
    "$x_i$에 대한 여러 함수들로 식을 표현하는 것이다. step function과 비슷한 형태다. 이때 각 함수 $b_k(x_i)$가 어떠한 형태일지-1차항인지 2차항도 포함인지-는 분석자가 미리 정해놓기 때문에 여러가지 개념을 포함할 수 있다. 가령 이 $b_i(x_i)$를 각각의 예측변수로 생각하면, standard linear model로 생각할 수 있다.\n",
    "<br> 이제 대표적인 basis function인 regression splines를 살펴보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 7.4 Regression Spines\n",
    "\n",
    "## 7.4.1 Piecewise Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piecewise Polynomial Regression: 전체 X의 범위에 고차항의 다항적합을 하는 것이 아니라, 몇개의 범주에 상대적으로 낮은 차수의 적합을 따로따로 한다. ex) 각 범주마다 $y_i = β_0 + β_1x_i + β_2x_i^2 + β_3x_i^3 +ϵ_i$를 적합하는 것이다. **이때 각 범주마다 회귀계수는 각 범주마다 다르다.** 각 범주가 바뀌는 지점을 **knots**라 부른다.\n",
    "\n",
    "예를 들어 knot가 상수 c 지점에서 하나 뿐이면, piecewise cubic(3차) polynomial식은 다음과 같다.\n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486465-6b6fe564-1716-11e8-8f1f-3b634091f6f4.PNG\" style=\"zoom:77%;\"/>\n",
    "\n",
    "$(x_i<c)$와 $(x_i≥c)$의 2개의 subset에 두번의 다항적합을 한것이라 보면 된다. 각 계수들은 지금까지와 같이 least sqaure로 적합. \n",
    "<br>이때, 모델의 자유도는 추정할 계수들의 수, 즉 4개씩 2범주로,  $df=8$이 된다.\n",
    "\n",
    "그러나 다른 제약을 걸지 않은 상태로는 다음과 같은 모델이 만들어진다.\n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486460-6ae9f17a-1716-11e8-8c02-8ce4b3531244.PNG\">\n",
    "50세를 기점으로 점프했다. 얼마나 비합리적인 모델인가. 그래서 우리는 제약을 걸어주기 위해 추가적인 방법을 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.2 Constraints and Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 비합리적 모델이 나오지 않도록, **1) 적합된 각 곡선이 서로 연결(continuous)되어야 한다**는 제약을 추가하자. \n",
    "<br>이에 따라 곡선들을 연결하면\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486456-6aaebe84-1716-11e8-8d93-9e9ae6b9838c.PNG\">\n",
    "\n",
    "잘 연결되었지만, 여전히 V자로 꺾여있다. 이 데이터에서는 V가 완만하지만, 단순 연결만 조건으로 걸면 다른 경우에는 뾰족한 V가 나올수도 있다. <br>따라서 새로운 제약 두개를 추가하자. **2) age=50의 knots에서 1차 미분 가능해야 한다 3) 2차 미분 가능해야 한다!** \n",
    "<br><br>이러면 연결되어 있는 매끄러운 곡선이 나온다. \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486453-6a70c4a8-1716-11e8-8295-6d69ee02806c.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 제약은 자유도(degree of freedom, 추정하고자 하는 회귀계수의 수)를 하나씩 잃는 것을 의미한다. \n",
    "<br>맨 위의 piecewise cubic에서는 자유도가 8이었지만, 3개의 제약으로 인해, Cubic Spline은 자유도가 5가 된다.\n",
    "<br><br>\n",
    "d차 적합을 하며 K개의 knots가 있는 regressoin spline의 경우 $(d+1)(K+1)−dK$의 자유도를 갖는다 보면 된다. (K개 knote니까 구간은 K+1개로 나뉘고, knot마다‘연속 + d-1차까지 미분가능'이라는 제약이 들어감) 이때, 상수항을 빼고 세면 $d+K$.\n",
    "<br><br> 일반화하면, d차 spline적합은 1) 각 piecewise에서 d차 다항적합을 하고, 2) 각 knot에서 (d-1)차까지의 미분가능\n",
    "<br>즉 매끄럽게 연속적이어야 한다는 제약이 붙는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.3 The Splines Basis Representation\n",
    "\n",
    "그렇다면 d차 적합을 하면서 d-1차까지의 미분가능이라는 제약을 어떻게 충족할 수 있는가?\n",
    "<br>이는 앞에서 다루었던 Basis function을 통해 알 수 있다. \n",
    "<img src= \"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/7.4.3.JPG?raw=true\"  style=\"zoom:67%;\"/>\n",
    "K개의 knots가 있는 cubic spline도 역시 b1,..,bk+3이 선택된다면 위 식 같이 basis function의 형태로 표현할 수 있다. \n",
    "<br><br>그럼 cubic spline을 어떻게 저렇게 표현할 수 있을까? 가장 대표적인 방법은 x,x2,x3만 있는 삼차 다항식에서 각 knot마다 truncated power basis function을 추가하는 것이다. truncated power basis function는 다음과 같다.\n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486452-6a3dbbda-1716-11e8-849c-932761aa4ee1.PNG\"  style=\"zoom:77%;\" />\n",
    "ξ는 knot를 의미. 즉 ξ에서 2차 미분(‘d-1’차 미분) 까지 미분이 가능하다는 것을 의미한다. \n",
    "<br>따라서 K개의 knots가 있을때 이를 다음과 같이 표현하게 된다. \n",
    "<img src=\"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/7.4.3.2.JPG?raw=true\" style=\"zoom:45%;\"/>\n",
    "<br>따라서 K개의 knots의 cubic spline적합은, $X,X^2,X^3,h(X,ξ_1),...,h(X,ξ_K)$의 K+3개의 예측변수로 least square적합을 한 것이다. \n",
    "<br>절편항을 생각하면 총 K+4개의 회귀계수를 추정해야 한다. 따라서 K개의 knots의 cubic spline은 $df=K+4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 이렇게 구한 cubic spline은 Boundary(양 끝, 처음과 마지막 knot 밖)에서 신뢰구간이 넓어져, 예측의 정확도가 떨어진다. (flexibility up, Variance up)<br> 직관적으로, 전 구간에서 모든 샘플을 사용해 D차 커브를 ﬁtting하는 D차 다항식 ﬁtting에 비해, D차 spline은 knot로 나뉜 각 구간 내의 제한된 샘플을 사용해 D차 커브를 ﬁtting 한다. 따라서 데이터가 거의 없는 Boundary에서는 제한된 샘플로 많은 계수를 추정해야 하므로, 그 구간에서의 추정치의 분포가 자유도가 더 낮은 t분포를 따르니 신뢰구간이 더 클 수 밖에 없다. \n",
    "<br><br>그래서 spline의 경우 끝단에서 선형 적합을 하여 이러한 문제를 완화하고자 한다. 이를 **Natural Cubic Spiline**이라 부른다. \n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486451-6a084090-1716-11e8-9a75-7bd49379e59f.PNG\" style=\"zoom:77%;\" />\n",
    "신뢰구간 범위가 좁아졌다. 이 경우 boundary에서 1차적합을 하기에, 각각 자유도 2씩을 잃어 $(K+4)-(2X2)$, 즉 $K$의 자유도를 갖게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.4 Choosing the Number and Location of the Knots\n",
    "\n",
    "그렇다면 몇개의 knots를 어디에 설정할지는 어떻게 정할까?\n",
    "- 자유도, 몇개의 knot를 설정할지를 정하는 데는 앞에서 나왔던 Cross-validation을 사용한다. CV error값이 가장 낮은 자유도의 갯수를 사용하는 것! 그림의 예시는 k-fold CV이고, 둘다 df=4가 적당해 보인다. \n",
    "<img src=\"https://github.com/esj205/ESC_ML_STUDY/blob/master/LectureNote/pic/7.4.4.JPG?raw=true\" style=\"zoom:67%;\" />\n",
    "- 그럼 어디에 설정해야 할까? 데이터가 급변하는 지점에는 flexible 할 수 있게 knot를 많이 배정하고, 아닌 부분에는 적은 수를 쓰면 좋을 것이다. 하지만 예측변수가 많을 경우에 어느 부분이 그러한지 알기 힘들 수도 있다. \n",
    "<br>따라서 좀더 단순한 방법을 쓰자. 자유도를 설정하고, 이에 따라 균등한 quantile에 knot를 배정하는 것이다! 다음은 자유도 4의 경우다(intercept 포함해서 생각하면 5). <br>\n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486450-69cc894c-1716-11e8-8a56-0e81c3dac086.PNG\" style=\"zoom:77%;\" />\n",
    "<br>자유도 4를 지정한 경우 natural cubic의 knot는 5개이기 때문에, boundary knot 2개를 제외하고, 3개의 knot들이 quantile에 따라 생겼다. \n",
    "<br> 즉, 25th, 50th, 75th Q가 knot로 배정된 것이다. 이를 통해 데이터가 많은 부분에선 많은 knot들을 배정하여 잠재적으로 가질수 있는 non-linear형태를 반영하고, 자료들이 많지 않은 부분에선 적은 수의 knot들을 배정하려는 전략을 어느 정도 달성할 수 있다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.5 Comparison to Polynomial Regression\n",
    "\n",
    "많은 경우 regression spline이 다항회귀보다 좋은 결과를 낸다.다항회귀는 flexiblility를 높이려면 차수를 지나치게 높여야 하는 반면, regression spline은 차수는 유지하면서 knot의 수를 늘려서 flexibility를 높일 수 있기 때문이다. 그래서 보통 regressoin spline이 안정된 추정을 한다. \n",
    "<br><br>밑의 그림은 natural cubic spline과 15차항 적합을 한 polynomial regression을 비교한 것이다. 다항회귀는 지나친 flexibility로 끝쪽에서 회귀선이 요동친다.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/31824102/36486449-6991b966-1716-11e8-9be7-9deee4766b52.PNG\" style=\"zoom:77%;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.5 Smoothing Spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **7.5.1 Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관측된 데이터에 잘 맞는 어떤 함수를 찾는 것이 목표! 근데 제한없이 단순히 RSS를 최소화하는 함수를 찾는 것은 overfitting이 발생할 수 있다. 극단적으로 개별 data를 하나의 knot로 보는 등 모든 data를 통과하는 함수는 과적합 발생\n",
    "우리가 찾는 건 RSS를 최소화하면서 동시에 과적합하지 않는 smooth 함수!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothing spline은 모든 data를 knot로 간주하면서 overfitting을 방지하는 spline이라고 생각하면 된다. 기존 RSS에 regularization term을 도입하여 그 값을 최소로 하는 함수를 찾는 것이고 해당 함수를 Smoothing spline이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수를 찾는 식은 다음과 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum_{i=1} ^{n} (y _{i} -g(x _{i} )) ^{2} + λ \\int _{} ^{} {g ^{''}(t) ^{ 2}dt }$ \n",
    "<br> 6장의 Ridge Lasso처럼 Loss + Penalty 형태!\n",
    "\n",
    "λ(lambda)는 음수가 아닌 tuning parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g의 2차 도함수는 기울기(g의 1차 도함수)가 변하는 정도(양) 즉 roughness의 측도이므로 그 적분값은 전체 범위에 걸친 기울기 총 변화량을 나타낸다. g가 smooth하면 적분값이 작을 것이고 g가 변동이 심한 경우는 큰 값을 가질 것이다. 이때 +/- 부호를 고려해서 제곱이 붙어있는 걸 볼 수 있다.\n",
    "λ가 0일때 penalty항이 없어지면서 기존 RSS와 똑같은 형태가 되며 g는 train set을 반영하려는 꾸불꾸불한 함수가 될 것이다. 반대로 λ가 커질수록 g는 선형 최소제곱선에 가까워질 것이다. 즉 λ값이 smoothing spline의 bias-variance tradeoff 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 최소로 하는 함수 g는 특별한 성질을 지닌다.(고 합니다)\n",
    "n개의 x값들에 knot가 있는 piecewise cubic polynomial + boundary 이외의 영역은 linear -->> 즉 natural cubic spline이다\n",
    "앞에서 배운 natural cubic spline과 완전히 같은 함수가 아닌 shrink된 함수라고 한다.(λ값에 따라 결정)\n",
    "Smoothing spline은 모든 input에 대해 knot를 가지고 있는 natural cubic spline을 shrinkage penalty를 hyperparameter λ​를 통해 조절해서 variance를 줄여주는 모델로도 볼 수 있는 것이다 (overfitting 억제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **7.5.2 Choosing λ**\n",
    "\n",
    "smoothing spline이 모든 x값들에서 knot를 갖는 natural cubic spline이라면 너무 큰 flexibility(또는 자유도)를 갖는 건 아닐까??\n",
    "\n",
    "<br>Tuning parameter λ가 이전 식에서 roughness를(2차 도함수의 적분) 제어함으로써(smooth) 유효자유도(effective degree of freedom)를 컨트롤. 구체적으로, λ가 0~무한대로 증가하면서 effective degree of freedom은 n에서 2로 감소."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why \"effective degrees of freedom\"?\n",
    "보통 자유도는 추정해야 할 모수의 개수를 말한다. smoothing spline은 명목상 자유도가 n이지만 이들은 심하게 제한/축소된다. 따라서 같은 n개의 모수이더라도 shrink 정도에 따라 flexibility가 다르니까 이를 따로 표현하고자 하는 것이다. effective df는 smoothing spline의 유연성을 의미한다. 높을수록 유연하다(높은 분산, 낮은 편향) λ에 의해 제한을 받으므로 명목상의 자유도보다 작은 dfλ를 갖게 된다고 이해함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ hat{g_{λ}}=S _{λ} y $\n",
    "<br> 회귀분석에서 y_hat = Hy 와 같은 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 g^는 x1,..,xn까지에 대해 적합된 n개의 적합값이다. Sλ는 y를 적합값(hat( 𝑔λ ))으로 표현하는 n X n의 matrix이다. 이때, 유효자유도는 dfλ=tr(Sλ)로 정의한다. 즉 대각원소들의 합이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression spline과 다르게 Smoothing spline은 적합하는 데 매듭의 수는 n개이고 그 위치 또한 알고 있다. 다만 λ를 결정하는 문제가 남아있다.\n",
    "1) Cross Validation - 교차검증된 RSS가 가능한 한 작게 하는 lambda 값을 찾을 수 있다.\n",
    "2) LOOCV(leave-one-out) - i번째 관측치 제외한 나머지 data로 적합. i번째 관측치를 해당 함수에 넣어서 결과 예측. 이 과정을 data 갯수만큼 반복. 다들 기억하시죠?? 모든 데이터에 대한 원래 적합만을 이용하여 쉽게 계산할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Lab7-5-0.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Lab7-5-1.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빨간 선은 유효자유도 16 가지는 smoothing spline이고 파란 선은 LOOCV를 이용하여 λ를 얻은 smoothing spline이다.(dfλ=6.8)\n",
    "거의 똑같다. 이 경우 단순한 모델로 충분히 설명이 되기 때문에 유효자유도가 더 작은 적합이 더 선호된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.6 Local Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification에서 배웠던 KNN과 유사한 개념. 특정 target을 잡고 그 주변 training data만을 사용하여 fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Local Regression Algorithm]\n",
    "\n",
    "<br> 1. $ x_{i}$가 $ x_{0}$에 가장 가까운 s(=k/n)개의 data를 모은다.\n",
    "<br> 2. 모은 점들에 가중치 $K_{i0}$=K($ x_{i}$,$ x_{0}$)를 할당한다. $ x_{0}$에 가까울수록 높은 가중치. s개의 data를 제외한 모든 점은 가중치가 0이다.\n",
    "<br> 3. 가중치를 이용해서 다음의 식을 최소로 하는 계수들을 찾음으로써 $ x_{i}$에 $y_{i}$의 가중 최소제곱회귀를 적합한다.\n",
    "\n",
    "$ \\sum _ {i=1} ^{n} K _ {i0} (y _ {i} - β  _ {0} - β  _ {1} x _ {i} ) ^{2}$\n",
    "\n",
    "<br> 4. $x_{0}$에서 적합한 값은 다음과 같이 주어진다.\n",
    "\n",
    "hat_f($x_{0}$)= hat_$β_{0}$ + hat_$β_{1}x_{0}$\n",
    "\n",
    "해당 과정을 모든 x 수준에 대해 반복한다. 각 적합값을 구할때마다 모든 traing data가 필요하기에, memory based 방식이라고도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Lab7-6-1.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇 가지 선택해야 할 것들이 있다.\n",
    "- 가중치 함수 K 정의(노란 bell-shaped curve 분포)\n",
    "- Step 3에서 선형, 상수 또는 이차회귀 적합 여부( 상기의 식은 선형회귀)\n",
    "- 가장 중요한, s 값 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "span s는 smoothing spline에서 tuning parameter lambda와 같은 역할을 하며 비선형적합의 유연성을 제어한다.(overfitting 방지)\n",
    "s값이 작으면 local의 작은 부분만 볼 것이기에 꾸불꾸불하게 fitting될 것이고\n",
    "반대로 s값이 크면 전체 데이터 중 많은 부분을 포함하기에 그보다는 smooth할 것이다.\n",
    "적절한 s값은 lambda와 마찬가지로 cross-validation을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설명변수가 여러 개인 상황에서, 다른 변수들에 대해서는 전역적이지만 시간과 같은 변수에는 국소적으로 local regression을 활용할 수 있다.\n",
    "최근에 수집된 데이터에 모델을 적응시키는 데 유용하다고 한다.\n",
    "여러 변수가 있을 때 단순히 nearest neighbor를 모아서 그 데이터들에 대해 fitting을 하면 된다. 다만 차원이 3,4,차원만 되어도 가까운 train data가 매우 적기 때문에 성능이 떨어진다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.7 Generalized Additive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 때까지 7장에서 살펴본 내용은 설명변수 X 하나에 대해서 반응변수 Y를 유연하게(비선형) 예측하는 여러 방법들을 살펴보았다. 단순선형회귀의 확장으로 볼 수도 있다.\n",
    "<br> 여기서는 여러 설명변수($ X_{1} $ ~ $ X_{p} $)를 기반으로 Y를 예측하는 issue ; 다중선형회귀의 확장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반화가법모델(GAMs)은 additivity를 유지하면서 각 변수에 non-linear하게 fitting하는 방법. 양적/질적 변수 모두 적용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7.7.1 회귀문제에 대한 GAMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중선형회귀모델\n",
    "<br>\n",
    "$y_{i}$ = $β_{0}$ + $β_{1}$$x_{i1}$+ ... + $β_{p}$$x_{ip}$ + $ϵ_{i}$\n",
    "\n",
    "비선형함수 f로 확장(대체)\n",
    "<br>\n",
    "$y_{i}$ = $β_{0}$ + $f_{1}$($x_{i1}$)+ ... + $f_{p}$($x_{ip}$) + $ϵ_{i}$\n",
    "\n",
    "<br>\n",
    "GAMs의 한 예시로서 각 X 수준에 대해 f값 계산하고 모두 더하기 때문에 additive라고 함(interaction term 없음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAM은 지금까지 1차원 변수에 대해 다룬 여러 방법들을 building block으로써 활용하여 가법적인 모델을 만들수 있게 해준다.\n",
    "<br>\n",
    "wage =$β_{0}$ + $f_{1}$(year) + $f_{2}$(age) + $f_{3}$(education) + ϵ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Lab7-7-1.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "age, education이 고정인 상황에서 year와 함께 wage는 증가하는 경향이 있다.\n",
    "한편 year, educaiton이 고정일 때 중간 정도의 age에서 wage가 가장 높게 나타났다.\n",
    "마지막으로 year, age가 고정일 때 education이 증가하면서 wage 역시 증가하는 모습을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "natural spline과 달리(최소제곱법) smoothing spline의 경우에 GAM fitting은 간단하지 않다. 왜냐면 penalty항이 존재하니까 최소제곱이 사용될 수 없기 때문. 이때는 다른 예측 변수들을 고정시킨 채 순서대로 변수들을 업데이트 시키는 backfitting이라는 방법을 사용한다. 이는 각 변수를 적합할때 해당 변수를 제외한 나머지 변수들의 partial residual에 적합을 통해서 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Lab7-7-2.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 두 그림은 거의 차이가 없다. 사실 대부분의 경우, natural spline과 smoothing spline은 거의 결과차이가 없다. 또한, GAMs에서 위의 두 방식이 아닌 다른 방식을 사용해도 물론 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GAMs의 장단점 (양적/질적 변수 모두 적용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 각 X 변수에 대해 비선형 함수를 fitting하여 표준 선형회귀로는 놓칠 비선형 관계를 모델링할 수 있다. 각 변수에 대해 일일이 변환을 할 필요가 없다.\n",
    "2) non linear fitting은 Y를 더 정확하게 예측할 가능성\n",
    "3) additive model이라서 Y에 대한 각 X 수준의 영향을 알 수 있다.(holding other things constant) 추론에 강점\n",
    "4) 각 X 변수에 대한 함수 f의 smoothness를 자유도로 나타낼 수 있다.\n",
    "5) additive라는 제한. 즉 interaction effect를 놓칠 수 있다. 그러나 선형회귀에서처럼 X1 * X2 형태의 추가적인 설명변수를 포함해서 수동으로 모델에 추가할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7.7.2 분류문제에 대한 GAMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편의상 Y는 0 또는 1\n",
    "<br>\n",
    "p(X) = Pr(Y=1 | X)\n",
    "\n",
    "<br>로지스틱 회귀모델\n",
    "<br>$log( { p(X)}/{1-p(X) })$ = $β_{0}$ + $β_{1}$$x_{1}$+ ... + $β_{p}$$x_{p}$\n",
    "\n",
    "<br> 로지스틱 회귀 GAM (non-linear 확장)\n",
    "<br>$log( { p(X)}/{1-p(X) })$ = $β_{0}$ + $f_{1}$($x_{1}$)+ ... + $f_{p}$($x_{p}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$log( { p(X)}/{1-p(X) })$  = $β_{0}$ + $β_{1}$ x year + $f_{2}$(age) + $f_{3}$(education)\n",
    "<br> 이때 p(X) = Pr(wage>250|year,age,education)\n",
    "<br> year는 단순선형회귀, age는 smoothing spline, education은 step function 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/Lab7-7-3.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "additive의 특성으로 각 X 변수들의 영향을 개별적으로 살펴볼 수 있는데 year가 특히 income에 영향을 미치지 않는다는 걸 알 수 있다. age, education 패널의 경우 regression part 그래프와 어느 정도 유사하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
